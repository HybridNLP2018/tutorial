{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-capturing-word-embeddings.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "aTTPhI_liU-u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generate word embeddings using Swivel\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this notebook we show how to generate word embeddings based on the K-Cap corpus using the [Swivel algorithm](https://arxiv.org/pdf/1602.02215). In particular, we reuse the implementation included in the [Tensorflow models repo on Github](https://github.com/tensorflow/models/tree/master/research/swivel) (with some small modifications)."
      ]
    },
    {
      "metadata": {
        "id": "LPx_bqtS7OCj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download a small text corpus\n",
        "First, let's download a corpus into our environment. We will use the `gutenberg` corpus, consisting of a few public-domain works published as part of the Project Gutenberg."
      ]
    },
    {
      "metadata": {
        "id": "3CQi8I19iU-x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xSLlCB0ui_mU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zc-8xkaofomg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "175b02b5-62b0-4ef5-a539-9b7487e9ec0f"
      },
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mgutenberg\u001b[0m/        \u001b[01;34msample_data\u001b[0m/  swivel.zip  wget-log.1\n",
            "\u001b[01;34miswc18-tutorial\u001b[0m/  \u001b[01;34mswivel\u001b[0m/       wget-log    wget-log.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UjX5TGjMeLT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f1b415a4-640c-4804-ecb4-dbdcd3c32436"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rdenaux/iswc18-tutorial.git"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'iswc18-tutorial'...\n",
            "remote: Counting objects: 26, done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 26 (delta 1), reused 23 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (26/26), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lDRsx2zkjshl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a9a57c5a-1669-4ff2-a24b-30a3b6457f50"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "lSNfGrTQ7qiN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's verify the corpus that was downloaded. It should be in the path specified below."
      ]
    },
    {
      "metadata": {
        "id": "CUFY-Sl8lFDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f65f5ca9-de04-45af-e81f-2f180234cbfa"
      },
      "cell_type": "code",
      "source": [
        "%ls '/root/nltk_data/corpora/gutenberg/'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "austen-emma.txt          carroll-alice.txt        README\n",
            "austen-persuasion.txt    chesterton-ball.txt      shakespeare-caesar.txt\n",
            "austen-sense.txt         chesterton-brown.txt     shakespeare-hamlet.txt\n",
            "bible-kjv.txt            chesterton-thursday.txt  shakespeare-macbeth.txt\n",
            "blake-poems.txt          edgeworth-parents.txt    whitman-leaves.txt\n",
            "bryant-stories.txt       melville-moby_dick.txt\n",
            "burgess-busterbrown.txt  milton-paradise.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hrh3N-bq72We",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the corpus consists of various books, one per file. Most word2vec implementations require you to pass a corpus as a single text file. We can issue a few commands to do this by concatenating all the `txt` files in the folder into a single `all.txt` file, which we will use later on."
      ]
    },
    {
      "metadata": {
        "id": "43SExjL46jdZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b85851ce-5b46-45a0-edb2-b03ba089cbdb"
      },
      "cell_type": "code",
      "source": [
        "%cd '/root/nltk_data/corpora/gutenberg/'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/nltk_data/corpora/gutenberg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ju7vkSHcNmcS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A couple of the files are encoded using iso-8859-1 or binary encodings, which will cause trouble later on, so we rename them to avoid including them into our corpus."
      ]
    },
    {
      "metadata": {
        "id": "6UxGMz7ONMnf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv chesterton-ball.txt chesterton-ball.badenc-txt\n",
        "!mv milton-paradise.txt milton-paradise.badenc-txt\n",
        "!mv shakespeare-caesar.txt shakespeare-caesar.badenc-txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OxaqMwgE6vhp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat *.txt >> all.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y0O7gAKR63UC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76ac7e8d-f50c-4b36-e1d1-dea8817fde96"
      },
      "cell_type": "code",
      "source": [
        "%ls -lah '/root/nltk_data/corpora/gutenberg/all.txt'"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 11M Sep 14 09:04 /root/nltk_data/corpora/gutenberg/all.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lf2qQSE98VuR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The full dataset is about 12MB, this is sufficient for demonstration purposes. In reality, you want to use very large corpora to train high-quality word embeddings. Since we now have a corpus to train the word embeddings on, we can go back to the initial folder."
      ]
    },
    {
      "metadata": {
        "id": "5V-SrnQ57Dm6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39d7284c-4ae4-4387-9d61-25dba024e911"
      },
      "cell_type": "code",
      "source": [
        "%cd '/content'"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1PyH6EI783Kw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download `swivel`, an algorithm for learning word embeddings\n",
        "Now that we have a corpus, we need an (implementation of an) algorithm for learning embeddings. There are various libraries and implementations for this:\n",
        "  * [word2vec](https://pypi.org/project/word2vec/) the system proposed by Mikolov that introduced many of the techniques now commonly used for learning word embeddings. It directly generates word embeddings from the text corpus by using a sliding window and trying to predict a target word based on neighbouring context words.\n",
        "  * [GloVe](https://github.com/stanfordnlp/GloVe) an alternative algorithm by Pennington, Socher and Manning. It splits the process in two steps: \n",
        "    1. calculating a word-word co-occurrence matrix \n",
        "    2. learning embeddings from this matrix\n",
        "  * [FastText](https://fasttext.cc/) is a more recent algorithm by Mikolov et al (now at Facebook) that extends the original word2vec algorithm in various ways. Among others, this algorithm takes into accout subword information.\n",
        "  \n",
        "In this tutorial we will be using [Swivel](https://github.com/tensorflow/models/tree/master/research/swivel) an algorithm similar to GloVe, which makes it easier to extend to include both words and concepts. As with GloVe, Swivel first extracts a word-word co-occurence matrix from a text corpus and then uses this matrix to learn the embeddings.\n",
        "\n",
        "Let's download and unzip the swivel code first."
      ]
    },
    {
      "metadata": {
        "id": "XnCd32Jblcos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5ce54d22-d1b9-4e76-d71b-b312c616b6cc"
      },
      "cell_type": "code",
      "source": [
        "!wget http://expertsystemlab.com/hybridNLP18/swivel.zip"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to ‘wget-log.2’.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BHE4yQO5oQwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "1622f800-1364-4fe8-9284-e046a4ba517f"
      },
      "cell_type": "code",
      "source": [
        "!unzip swivel.zip"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  swivel.zip\n",
            "  inflating: swivel/analogy.cc       \n",
            "  inflating: swivel/distributed.sh   \n",
            "  inflating: swivel/eval.mk          \n",
            "  inflating: swivel/fastprep.cc      \n",
            "  inflating: swivel/fastprep.mk      \n",
            "  inflating: swivel/glove_to_shards.py  \n",
            "  inflating: swivel/nearest.py       \n",
            "  inflating: swivel/prep.py          \n",
            "  inflating: swivel/README.md        \n",
            "  inflating: swivel/swivel.py        \n",
            "  inflating: swivel/text2bin.py      \n",
            "  inflating: swivel/vecs.py          \n",
            "  inflating: swivel/wordsim.py       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RrqdpuIyHphq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm swivel/*\n",
        "!rm swivel.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bYPOStSgH7-F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d3970495-a21c-46cf-c1ca-cc03de83d4b3"
      },
      "cell_type": "code",
      "source": [
        "!grep -n .tell swivel/prep.py"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94:  nbytes = lines.tell()\r\n",
            "104:      pos = lines.tell()\r\n",
            "146:  nbytes = lines.tell()\r\n",
            "206:      #pos = lines.tell()\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7xppOkdyiU-1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Learn embeddings\n",
        "\n",
        "### Generate co-occurrence matrix using Swivel `prep`\n",
        "\n",
        "Call swivel's `prep` command to calculate the word co-occurrence matrix. We use the `%run` magic command, which runs the named python file as a program, allowing us to pass parameters as if using a command-line terminal.\n",
        "\n",
        "We set the `shard_size` to 512 since the corpus is quite small. For larger corpora we could use the standard value of 4096."
      ]
    },
    {
      "metadata": {
        "id": "1ZPID3mVU8Ed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "22b54eeb-5490-49aa-e330-c19de421a1ad"
      },
      "cell_type": "code",
      "source": [
        "!df -h"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay          40G  7.6G   30G  21% /\n",
            "tmpfs           6.4G     0  6.4G   0% /dev\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\n",
            "tmpfs           6.4G     0  6.4G   0% /opt/bin\n",
            "/dev/sda1        46G  8.4G   37G  19% /etc/hosts\n",
            "shm              64M     0   64M   0% /dev/shm\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Psu_5dOvXjru",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "c64f882f-0141-4826-c6e4-e3fe92170582"
      },
      "cell_type": "code",
      "source": [
        "!df -h"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay          40G  7.6G   30G  21% /\n",
            "tmpfs           6.4G     0  6.4G   0% /dev\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\n",
            "tmpfs           6.4G     0  6.4G   0% /opt/bin\n",
            "/dev/sda1        46G  8.5G   37G  19% /etc/hosts\n",
            "shm              64M     0   64M   0% /dev/shm\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MTSTLkQdX0tl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "1f49402f-072b-40ef-81c5-c6b966a9fd75"
      },
      "cell_type": "code",
      "source": [
        "%lsmagic"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/json": {
              "cell": {
                "prun": "ExecutionMagics",
                "file": "Other",
                "!": "OSMagics",
                "capture": "ExecutionMagics",
                "timeit": "ExecutionMagics",
                "script": "ScriptMagics",
                "pypy": "Other",
                "system": "OSMagics",
                "perl": "Other",
                "html": "DisplayMagics",
                "bash": "Other",
                "shell": "Other",
                "HTML": "Other",
                "python": "Other",
                "SVG": "Other",
                "javascript": "DisplayMagics",
                "bigquery": "Other",
                "js": "DisplayMagics",
                "writefile": "OSMagics",
                "ruby": "Other",
                "python3": "Other",
                "python2": "Other",
                "latex": "DisplayMagics",
                "sx": "OSMagics",
                "svg": "DisplayMagics",
                "sh": "Other",
                "time": "ExecutionMagics",
                "debug": "ExecutionMagics"
              },
              "line": {
                "psource": "NamespaceMagics",
                "logstart": "LoggingMagics",
                "popd": "OSMagics",
                "loadpy": "CodeMagics",
                "colors": "BasicMagics",
                "who_ls": "NamespaceMagics",
                "lf": "Other",
                "ll": "Other",
                "pprint": "BasicMagics",
                "lk": "Other",
                "ls": "Other",
                "save": "CodeMagics",
                "tb": "ExecutionMagics",
                "lx": "Other",
                "pylab": "PylabMagics",
                "killbgscripts": "ScriptMagics",
                "quickref": "BasicMagics",
                "magic": "BasicMagics",
                "dhist": "OSMagics",
                "edit": "KernelMagics",
                "logstop": "LoggingMagics",
                "gui": "BasicMagics",
                "prun": "ExecutionMagics",
                "debug": "ExecutionMagics",
                "page": "BasicMagics",
                "logstate": "LoggingMagics",
                "ed": "Other",
                "pushd": "OSMagics",
                "timeit": "ExecutionMagics",
                "set_env": "OSMagics",
                "rehashx": "OSMagics",
                "hist": "Other",
                "qtconsole": "KernelMagics",
                "rm": "Other",
                "dirs": "OSMagics",
                "run": "ExecutionMagics",
                "reset_selective": "NamespaceMagics",
                "pinfo2": "NamespaceMagics",
                "matplotlib": "PylabMagics",
                "unload_ext": "ExtensionMagics",
                "doctest_mode": "BasicMagics",
                "logoff": "LoggingMagics",
                "reload_ext": "ExtensionMagics",
                "pdb": "ExecutionMagics",
                "load": "CodeMagics",
                "lsmagic": "BasicMagics",
                "autosave": "KernelMagics",
                "cd": "OSMagics",
                "pastebin": "CodeMagics",
                "alias_magic": "BasicMagics",
                "cp": "Other",
                "autocall": "AutoMagics",
                "ldir": "Other",
                "bookmark": "OSMagics",
                "connect_info": "KernelMagics",
                "mkdir": "Other",
                "system": "OSMagics",
                "whos": "NamespaceMagics",
                "rmdir": "Other",
                "automagic": "AutoMagics",
                "store": "StoreMagics",
                "more": "KernelMagics",
                "shell": "Other",
                "pdef": "NamespaceMagics",
                "precision": "BasicMagics",
                "pinfo": "NamespaceMagics",
                "pwd": "OSMagics",
                "psearch": "NamespaceMagics",
                "reset": "NamespaceMagics",
                "recall": "HistoryMagics",
                "xdel": "NamespaceMagics",
                "xmode": "BasicMagics",
                "cat": "Other",
                "mv": "Other",
                "rerun": "HistoryMagics",
                "logon": "LoggingMagics",
                "history": "HistoryMagics",
                "pycat": "OSMagics",
                "unalias": "OSMagics",
                "env": "OSMagics",
                "load_ext": "ExtensionMagics",
                "config": "ConfigMagics",
                "profile": "BasicMagics",
                "pfile": "NamespaceMagics",
                "less": "KernelMagics",
                "who": "NamespaceMagics",
                "notebook": "BasicMagics",
                "man": "KernelMagics",
                "sx": "OSMagics",
                "macro": "ExecutionMagics",
                "clear": "KernelMagics",
                "alias": "OSMagics",
                "time": "ExecutionMagics",
                "sc": "OSMagics",
                "rep": "Other",
                "pdoc": "NamespaceMagics"
              }
            },
            "text/plain": [
              "Available line magics:\n",
              "%alias  %alias_magic  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %popd  %pprint  %precision  %profile  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %shell  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n",
              "\n",
              "Available cell magics:\n",
              "%%!  %%HTML  %%SVG  %%bash  %%bigquery  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%shell  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n",
              "\n",
              "Automagic is ON, % prefix IS NOT needed for line magics."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "8wi_izVOiU-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "597c927d-745b-4281-a7f1-645ef9fc70cc"
      },
      "cell_type": "code",
      "source": [
        "corpus_path = '/root/nltk_data/corpora/gutenberg/all.txt'\n",
        "coocs_path = '/content/gutenberg/coocs'\n",
        "shard_size = 512\n",
        "%run -i -e swivel/prep --input={corpus_path} --output_dir={coocs_path} --shard_size={shard_size}"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running with flags \n",
            "swivel/prep.py:\n",
            "  --bufsz: The number of co-occurrences to buffer\n",
            "    (default: '16777216')\n",
            "    (an integer)\n",
            "  --input: The input text.\n",
            "    (default: '')\n",
            "  --max_vocab: The maximum vocabulary size\n",
            "    (default: '1048576')\n",
            "    (an integer)\n",
            "  --min_count: The minimum number of times a word should occur to be included in\n",
            "    the vocabulary\n",
            "    (default: '5')\n",
            "    (an integer)\n",
            "  --output_dir: Output directory for Swivel data\n",
            "    (default: '/tmp/swivel_data')\n",
            "  --shard_size: The size for each shard\n",
            "    (default: '4096')\n",
            "    (an integer)\n",
            "  --vocab: Vocabulary to use instead of generating one\n",
            "    (default: '')\n",
            "  --window_size: The window size\n",
            "    (default: '10')\n",
            "    (an integer)\n",
            "\n",
            "tensorflow.python.platform.app:\n",
            "  -h,--[no]help: show this help\n",
            "    (default: 'false')\n",
            "  --[no]helpfull: show full help\n",
            "    (default: 'false')\n",
            "  --[no]helpshort: show this help\n",
            "    (default: 'false')\n",
            "\n",
            "absl.flags:\n",
            "  --flagfile: Insert flag definitions from the given file into the command line.\n",
            "    (default: '')\n",
            "  --undefok: comma-separated list of flag names that it is okay to specify on\n",
            "    the command line even if the program does not define a flag with that name.\n",
            "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\n",
            "    format.\n",
            "    (default: '')\n",
            "\n",
            "vocabulary contains 23040 tokens\n",
            "Computing co-occurrences: 230000..., last lid 0, sum(0)=591254.046428\n",
            "writing shard 2025/2025\n",
            "Wrote vocab and sum files to /content/gutenberg/coocs\n",
            "Wrote vocab and sum files to /content/gutenberg/coocs\n",
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5fyTfMneQnQc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We see that first, the algorithm determined the **vocabulary** $V$, this is the list of words for which an embedding will be generated. Since the corpus is fairly small, so is the vocabulary, which consists of only about 23K words (large corpora can result in vocabularies with millions of words).\n",
        "\n",
        "The co-occurrence matrix is a sparse matrix of $|V| \\times |V|$ elements. Swivel uses shards to create submatrices of $|S| \\times |S|$, where $S$ is the shard-size specified above. In this case, we have 2025 sub-matrices.\n",
        "\n",
        "All this information is stored in the output folder we specified above. It consists of  2025 files, one per shard/sub-matrix and a few additional files:"
      ]
    },
    {
      "metadata": {
        "id": "3eiJEg8mSOlJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d04a0a8d-3d38-4d28-dc86-d1b55809bcca"
      },
      "cell_type": "code",
      "source": [
        "%ls /content/gutenberg/coocs/ | head -n 10"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "col_sums.txt\n",
            "col_vocab.txt\n",
            "row_sums.txt\n",
            "row_vocab.txt\n",
            "shard-000-000.pb\n",
            "shard-000-001.pb\n",
            "shard-000-002.pb\n",
            "shard-000-003.pb\n",
            "shard-000-004.pb\n",
            "shard-000-005.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AMm9rXSbiU-8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The expected output is:\n",
        "\n",
        "    vocabulary contains 23040 tokens\n",
        "\n",
        "    writing shard 2025/2025\n",
        "    done!\n",
        "    \n",
        "The `prep` step does the following:\n",
        "  - it uses a basic, white space, tokenization to get sequences of tokens\n",
        "  - in a first pass through the corpus, it counts all tokens and keeps only those that have a minimum frequency (5) in the corpus. Then it keeps a multiple of the `shard_size` of that. The tokens that are kept form the **vocabulary** with size $v = |V|$.\n",
        "  - on a second pass through the corpus, it uses a sliding window to count co-occurrences between the focus token and the context tokens (similar to `word2vec`). The result is a sparse co-occurrence matrix of size $v \\times v$.\n",
        "  - for easier storage and manipulation, Swivel uses *sharding* to split the co-occurrence matrix into sub-matrices of size $s \\times s$, where $s$ is the `shard_size`.\n",
        "  ![Swivel co-occurrence matrix sharding](https://github.com/hybridNLP2018/tutorial/blob/master/images/swivel-sharding.PNG?raw=1)\n",
        "  - store the sharded co-occurrence submatrices as [protobuf files](https://developers.google.com/protocol-buffers/)."
      ]
    },
    {
      "metadata": {
        "id": "o9tIXHWbiU-9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Learn embeddings from co-occurrence matrix\n",
        "With the sharded co-occurrence matrix it is now possible to learn embeddings:\n",
        " - the input is the folder with the co-occurrence matrix (protobuf files with the sparse matrix).\n",
        " - `submatrix_` rows and columns need to be the same size as the `shard_size` used in the `prep` step."
      ]
    },
    {
      "metadata": {
        "id": "nyk4vjv6iU--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "cd0b290c-81f2-4a6a-8a11-41c87301d035"
      },
      "cell_type": "code",
      "source": [
        "vec_path = '/content/gutenberg/txt/vec/'\n",
        "%run -i swivel/swivel --input_base_path={coocs_path} \\\n",
        "    --output_base_path={vec_path} \\\n",
        "    --num_epochs=20 --dim=300 \\\n",
        "    --submatrix_rows={shard_size} --submatrix_cols={shard_size}"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:local_step=39990 global_step=39990 loss=20.6, 98.7% complete\n",
            "INFO:tensorflow:local_step=40000 global_step=40000 loss=20.7, 98.8% complete\n",
            "INFO:tensorflow:local_step=40010 global_step=40010 loss=19.5, 98.8% complete\n",
            "INFO:tensorflow:local_step=40020 global_step=40020 loss=20.2, 98.8% complete\n",
            "INFO:tensorflow:local_step=40030 global_step=40030 loss=20.6, 98.8% complete\n",
            "INFO:tensorflow:local_step=40040 global_step=40040 loss=21.7, 98.9% complete\n",
            "INFO:tensorflow:local_step=40050 global_step=40050 loss=19.8, 98.9% complete\n",
            "INFO:tensorflow:local_step=40060 global_step=40060 loss=20.8, 98.9% complete\n",
            "INFO:tensorflow:local_step=40070 global_step=40070 loss=19.7, 98.9% complete\n",
            "INFO:tensorflow:local_step=40080 global_step=40080 loss=20.0, 99.0% complete\n",
            "INFO:tensorflow:local_step=40090 global_step=40090 loss=21.3, 99.0% complete\n",
            "INFO:tensorflow:local_step=40100 global_step=40100 loss=22.3, 99.0% complete\n",
            "INFO:tensorflow:local_step=40110 global_step=40110 loss=20.1, 99.0% complete\n",
            "INFO:tensorflow:local_step=40120 global_step=40120 loss=19.8, 99.1% complete\n",
            "INFO:tensorflow:local_step=40130 global_step=40130 loss=20.9, 99.1% complete\n",
            "INFO:tensorflow:local_step=40140 global_step=40140 loss=22.1, 99.1% complete\n",
            "INFO:tensorflow:local_step=40150 global_step=40150 loss=22.6, 99.1% complete\n",
            "INFO:tensorflow:local_step=40160 global_step=40160 loss=20.9, 99.2% complete\n",
            "INFO:tensorflow:local_step=40170 global_step=40170 loss=20.0, 99.2% complete\n",
            "INFO:tensorflow:local_step=40180 global_step=40180 loss=19.0, 99.2% complete\n",
            "INFO:tensorflow:local_step=40190 global_step=40190 loss=22.0, 99.2% complete\n",
            "INFO:tensorflow:local_step=40200 global_step=40200 loss=21.2, 99.3% complete\n",
            "INFO:tensorflow:local_step=40210 global_step=40210 loss=19.9, 99.3% complete\n",
            "INFO:tensorflow:local_step=40220 global_step=40220 loss=21.9, 99.3% complete\n",
            "INFO:tensorflow:local_step=40230 global_step=40230 loss=407.1, 99.3% complete\n",
            "INFO:tensorflow:local_step=40240 global_step=40240 loss=404.9, 99.4% complete\n",
            "INFO:tensorflow:local_step=40250 global_step=40250 loss=19.8, 99.4% complete\n",
            "INFO:tensorflow:local_step=40260 global_step=40260 loss=20.2, 99.4% complete\n",
            "INFO:tensorflow:local_step=40270 global_step=40270 loss=476.6, 99.4% complete\n",
            "INFO:tensorflow:local_step=40280 global_step=40280 loss=20.4, 99.5% complete\n",
            "INFO:tensorflow:local_step=40290 global_step=40290 loss=21.3, 99.5% complete\n",
            "INFO:tensorflow:local_step=40300 global_step=40300 loss=20.7, 99.5% complete\n",
            "INFO:tensorflow:local_step=40310 global_step=40310 loss=22.9, 99.5% complete\n",
            "INFO:tensorflow:local_step=40320 global_step=40320 loss=20.6, 99.6% complete\n",
            "INFO:tensorflow:local_step=40330 global_step=40330 loss=20.4, 99.6% complete\n",
            "INFO:tensorflow:local_step=40340 global_step=40340 loss=19.4, 99.6% complete\n",
            "INFO:tensorflow:local_step=40350 global_step=40350 loss=22.1, 99.6% complete\n",
            "INFO:tensorflow:local_step=40360 global_step=40360 loss=20.5, 99.7% complete\n",
            "INFO:tensorflow:local_step=40370 global_step=40370 loss=20.8, 99.7% complete\n",
            "INFO:tensorflow:local_step=40380 global_step=40380 loss=19.5, 99.7% complete\n",
            "INFO:tensorflow:local_step=40390 global_step=40390 loss=20.0, 99.7% complete\n",
            "INFO:tensorflow:local_step=40400 global_step=40400 loss=21.9, 99.8% complete\n",
            "INFO:tensorflow:local_step=40410 global_step=40410 loss=20.2, 99.8% complete\n",
            "INFO:tensorflow:local_step=40420 global_step=40420 loss=21.4, 99.8% complete\n",
            "INFO:tensorflow:local_step=40430 global_step=40430 loss=18.7, 99.8% complete\n",
            "INFO:tensorflow:local_step=40440 global_step=40440 loss=20.3, 99.9% complete\n",
            "INFO:tensorflow:local_step=40450 global_step=40450 loss=21.3, 99.9% complete\n",
            "INFO:tensorflow:local_step=40460 global_step=40460 loss=21.0, 99.9% complete\n",
            "INFO:tensorflow:local_step=40470 global_step=40470 loss=21.5, 99.9% complete\n",
            "INFO:tensorflow:local_step=40480 global_step=40480 loss=20.4, 100.0% complete\n",
            "INFO:tensorflow:local_step=40490 global_step=40490 loss=19.3, 100.0% complete\n",
            "INFO:tensorflow:local_step=40500 global_step=40500 loss=19.5, 100.0% complete\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oEDrblTXiU_B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This should take a few minutes, depending on your machine.\n",
        "The result is a list of files in the specified output folder, including:\n",
        " - checkpoints of the model\n",
        " - `tsv` files for the column and row embeddings."
      ]
    },
    {
      "metadata": {
        "id": "_BOhXpZiiU_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "2c5ceef0-5605-4046-85de-ea4e5a8fb056"
      },
      "cell_type": "code",
      "source": [
        "os.listdir(vec_path)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model.ckpt-26113.data-00000-of-00001',\n",
              " 'row_embedding.tsv',\n",
              " 'graph.pbtxt',\n",
              " 'model.ckpt-40500.meta',\n",
              " 'col_embedding.tsv',\n",
              " 'model.ckpt-40500.index',\n",
              " 'model.ckpt-40500.data-00000-of-00001',\n",
              " 'checkpoint',\n",
              " 'model.ckpt-26113.meta',\n",
              " 'model.ckpt-0.data-00000-of-00001',\n",
              " 'model.ckpt-26113.index',\n",
              " 'model.ckpt-0.meta',\n",
              " 'model.ckpt-0.index',\n",
              " 'events.out.tfevents.1536918848.0873453509b3']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "xI0JCPPpiU_G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert `tsv` files to `bin` file\n",
        "The `tsv` files are easy to inspect, but they take too much space and they are slow to load since we need to convert the different values to floats and pack them as vectors. Swivel offers a utility to convert the `tsv` files into a `bin`ary format. At the same time it combines the column and row embeddings into a single space (it simply adds the two vectors for each word in the vocabulary)."
      ]
    },
    {
      "metadata": {
        "id": "qn0FHiYJiU_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e04ce907-e52a-4beb-87c3-d27dbdc24fc7"
      },
      "cell_type": "code",
      "source": [
        "%run -i swivel/text2bin --vocab={vec_path}vocab.txt --output={vec_path}vecs.bin \\\n",
        "        {vec_path}row_embedding.tsv \\\n",
        "        {vec_path}col_embedding.tsv"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "executing text2bin\n",
            "merging files ['/content/gutenberg/txt/vec/row_embedding.tsv', '/content/gutenberg/txt/vec/col_embedding.tsv'] into output bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MrAWhqbViU_M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This adds the `vocab.txt` and `vecs.bin` to the folder with the vectors:"
      ]
    },
    {
      "metadata": {
        "id": "JX6hOIGqiU_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "45cf706b-4fc6-4985-b473-cff98e75bb67"
      },
      "cell_type": "code",
      "source": [
        "%ls {vec_path}"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\n",
            "col_embedding.tsv\n",
            "events.out.tfevents.1536918848.0873453509b3\n",
            "graph.pbtxt\n",
            "model.ckpt-0.data-00000-of-00001\n",
            "model.ckpt-0.index\n",
            "model.ckpt-0.meta\n",
            "model.ckpt-26113.data-00000-of-00001\n",
            "model.ckpt-26113.index\n",
            "model.ckpt-26113.meta\n",
            "model.ckpt-40500.data-00000-of-00001\n",
            "model.ckpt-40500.index\n",
            "model.ckpt-40500.meta\n",
            "row_embedding.tsv\n",
            "vecs.bin\n",
            "vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hY-1lWr3iU_R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Read stored binary embeddings and inspect them\n",
        "\n",
        "Swivel provides the `vecs` library which implements the basic `Vecs` class. It accepts a `vocab_file` and a file for the binary serialization of the vectors (`vecs.bin`)."
      ]
    },
    {
      "metadata": {
        "id": "MAB_mb9ViU_R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from swivel import vecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ifovawLuiU_U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "...and we can load existing vectors. Here we load some pre-computed embeddings, but feel free to use the embeddings you computed by following the steps above (although, due to random initialization of weight during the training step, your results may be different)."
      ]
    },
    {
      "metadata": {
        "id": "y4l1xtJNiU_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3e9ce650-e860-4a30-c7f9-70e71f663f5b"
      },
      "cell_type": "code",
      "source": [
        "#precomp_vec_path = 'corpora/kcap15-17/precomp-txt/vec/'\n",
        "#vec_path = precomp_vec_path # uncomment if you did not manage to train embeddings above\n",
        "vecs = vecs.Vecs(vec_path + 'vocab.txt', \n",
        "            vec_path + 'vecs.bin')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Opening vector with expected size 23040 from file /content/gutenberg/txt/vec/vocab.txt\n",
            "vocab size 23040 (unique 23040)\n",
            "read rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VjgSQtsPiU_Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, let's define a basic method for printing the `k` nearest neighbors for a given word:"
      ]
    },
    {
      "metadata": {
        "id": "pw4udZ1WiU_a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def k_neighbors(word, k=10):\n",
        "    res = vecs.neighbors(word)\n",
        "    if not res:\n",
        "        print('%s is not in the vocabulary, try e.g. %s' % (word, vecs.random_word_in_vocab()))\n",
        "    else:\n",
        "        for word, sim in res[:10]:\n",
        "            print('%0.4f: %s' % (sim, word))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s9ld45o3iU_e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And let's use the method on a few words:"
      ]
    },
    {
      "metadata": {
        "id": "2iZ9bExziU_e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "cf5d7d1d-713f-4532-ec82-de05727f869b"
      },
      "cell_type": "code",
      "source": [
        "k_neighbors('California')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0000: California\n",
            "0.4007: Pennsylvania,\n",
            "0.3726: wool\n",
            "0.3176: Clear\n",
            "0.2998: forenoon\n",
            "0.2880: lakes,\n",
            "0.2822: song,\n",
            "0.2791: lakes\n",
            "0.2786: Missouri,\n",
            "0.2732: life,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RfCmWTFsiU_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ee444bf9-571c-49a5-920a-970767ecc037"
      },
      "cell_type": "code",
      "source": [
        "k_neighbors('knowledge')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0000: knowledge\n",
            "0.3061: utter\n",
            "0.2724: stature,\n",
            "0.2605: kinds\n",
            "0.2522: Spirit;\n",
            "0.2414: imperfect\n",
            "0.2411: partial\n",
            "0.2399: Colonel's\n",
            "0.2279: wisdom,\n",
            "0.2238: mayest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iwO2zXERiU_o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "61be25c9-2982-4ad6-bea7-0cdedf3bb796"
      },
      "cell_type": "code",
      "source": [
        "k_neighbors('science')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0000: science\n",
            "0.3302: marine\n",
            "0.3165: matters.\n",
            "0.3127: pretensions\n",
            "0.3025: equality,\n",
            "0.2901: march\n",
            "0.2698: whales.\n",
            "0.2684: sentimental\n",
            "0.2623: falsely\n",
            "0.2591: generations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3Mcdr8dIiU_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ee8ddd19-8c29-4ffa-b54e-b37d258b45ae"
      },
      "cell_type": "code",
      "source": [
        "k_neighbors('national')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0000: national\n",
            "0.4614: prejudices\n",
            "0.3875: hero\n",
            "0.3769: buoyancy\n",
            "0.3386: habitual\n",
            "0.3348: importance.\n",
            "0.2786: conceal'd\n",
            "0.2770: reserve\n",
            "0.2615: Many\n",
            "0.2599: faults,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-Ze0ApZMluvX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "bf6bc2fb-40da-417d-97a0-e7e1e5b98b2f"
      },
      "cell_type": "code",
      "source": [
        "k_neighbors('conference')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0000: conference\n",
            "0.3847: afterwards,\n",
            "0.3199: private\n",
            "0.2983: added\n",
            "0.2583: opportunity\n",
            "0.2553: concluding\n",
            "0.2468: list\n",
            "0.2365: soon\n",
            "0.2356: friendly\n",
            "0.2229: horizontal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bJAlDOIZiU_z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we used swivel to generate word embeddings and we explored the resulting embeddings using `k neighbors` exploration. "
      ]
    }
  ]
}
