{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_vecsigrafo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "o5OAyd4nluKL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generate a Vecsigrafo using Swivel\n",
        "\n",
        "\n",
        "In this notebook we show how to generate a Vecsigrafo based on a subset of the [UMBC corpus](https://ebiquity.umbc.edu/resource/html/id/351/UMBC-webbase-corpus).\n",
        "\n",
        "We follow the procedure described in [Towards a Vecsigrafo: Portable Semantics in Knowledge-based Text Analytics](https://pdfs.semanticscholar.org/b0d6/197940d8f1a5fa0d7474bd9a94bd9e44a0ee.pdf) and depicted in the following figure:\n",
        "\n",
        "![Generic Vecsigrafo Creation](https://github.com/hybridNLP2018/tutorial/blob/master/images/generic-vecsigrafo-creation.png?raw=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xLwTh2Qhs5Lt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenization and Word Sense Disambiguation\n",
        "\n",
        "The main difference with standard swivel is that:\n",
        " - we use word-sense disambiguation on the text as a pre-processing step (Swivel simply uses white-space tokenization)\n",
        " - each 'token' in the resulting sequences is composed of a lemma and an optional concept identifier.\n",
        " \n",
        "### Disambiguators\n",
        "If we are going to apply WSD, we will need some disambiguator strategy. Unfortunately, there are not a lot of open-source high-performance disambiguators available. At [Expert System](https://www.expertsystem.com/), we have a [state-of-the-art disambiguator](https://www.expertsystem.com/products/cogito-cognitive-technology/semantic-technology/disambiguation/) that assings **syncon**s (our version of synsets) to lemmas in the text. \n",
        "\n",
        "Since Expert System's disambiguator and semantic KG are proprietary, in this notebook we will be mostly using WordNet (although we may present some results and examples based on Expert System's results). We have implemented a lightweight disambiguation strategy, proposed by [Mancini, M., Camacho-Collados, J., Iacobacci, I., & Navigli, R. (2017). Embedding Words and Senses Together via Joint Knowledge-Enhanced Training. CoNLL.](http://arxiv.org/abs/1612.02703), which has allowed us to produce disambiguated corpora based on WordNet 3.1.\n",
        "\n",
        "To be able to inspect the disambiguated corpus, let's make sure we have access to WordNet in our environment by executing the following cell.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7PGZ7dicIUAI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "37d1773b-149b-4065-ea58-76a216a5e98f"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "wn.synset('Maya.n.02')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('maya.n.02')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "bhQ3aE4oKnIp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenizations\n",
        "When applying a disambiguator, the tokens are no longer (groups of) words. Each token can contain different types of information, we generally keep the following token information:\n",
        "  * `t`: text, the original text (possibly normalised, i.e. lower-cased)\n",
        "  * `l`: lemma, the lemma form of the word\n",
        "  * `g`: grammar: the grammar type\n",
        "  * `s`: syncon (or synset) identifier\n",
        "  \n",
        "### Example WordNet\n",
        "\n",
        "We have included a small sample of our disambiguated UMBC corpus as part of our [GitHub tutorial repo](https://github.com/HybridNLP2018/tutorial). Execute the following cell to clone the repo, unzip the sample corpus and print the first line of the corpus"
      ]
    },
    {
      "metadata": {
        "id": "b0s_9qVzw5bd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "787ef925-4f14-4715-8e16-4595181acd10"
      },
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/HybridNLP2018/tutorial.git\n",
        "%cd /content/tutorial/datasamples/\n",
        "!unzip umbc_tlgs_wnscd_5K.zip\n",
        "toked_corpus = '/content/tutorial/datasamples/umbc_tlgs_wnscd_5K'\n",
        "!head -n1 {toked_corpus}\n",
        "%cd /content/"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "fatal: destination path 'tutorial' already exists and is not an empty directory.\n",
            "/content/tutorial/datasamples\n",
            "Archive:  umbc_tlgs_wnscd_5K.zip\n",
            "replace umbc_tlgs_wnscd_5K? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "the%7CGT_ART mayan%7Clem_Mayan%7CGT_ADJ%7Cwn31_Maya.n.03 image%7Clem_image%7CGT_NOU%7Cwn31_effigy.n.01 collection%7Clem_collection%7CGT_NOU%7Cwn31_collection.n.01 was%7Clem_be%7CGT_AUX%7Cwn31_embody.v.02 contributed%7Clem_contribute%7CGT_VER%7Cwn31_contribute.v.02 by%7Clem_by%7CGT_PRE%7Cwn31_aside.r.06 oberlin+college%7Clem_Oberlin+College%7CGT_NPR faculty%7Clem_faculty%7CGT_NOU%7Cwn31_staff.n.03 and%7CGT_CON library%7Clem_library%7CGT_NOU%7Cwn31_library.n.02 staff%7Clem_staff%7CGT_NOU%7Cwn31_staff.n.03 .%7CGT_PNT professor%7Clem_professor%7CGT_NOU%7Cwn31_professor.n.01 linda+grimm%7Clem_Linda+Grimm%7CGT_NPH %2C%7CGT_PNT associate+professor%7Clem_associate+professor%7CGT_NOU%7Cwn31_associate+professor.n.01 of%7CGT_PRE anthropology%7Clem_anthropology%7CGT_NOU%7Cwn31_anthropology.n.01 and%7CGT_CON project%7Clem_project%7CGT_NOU%7Cwn31_visualize.v.01 coordinator%7Clem_coordinator%7CGT_NOU%7Cwn31_coordinator.n.01 at%7Clem_at%7CGT_PRE%7Cwn31_at.n.02 oberlin%7Clem_Oberlin+College%7CGT_NPR %2C%7CGT_PNT explained%7Clem_explain%7CGT_VER%7Cwn31_excuse.v.03 the%7CGT_ART educational%7Clem_educational%7CGT_ADJ%7Cwn31_educational.a.01 goals%7Clem_goal%7CGT_NOU%7Cwn31_finish.n.04 for%7CGT_PRE this%7Clem_this%7CGT_ADJ online%7Clem_online%7CGT_ADJ%7Cwn31_on-line.a.01 project%7Clem_project%7CGT_NOU%7Cwn31_visualize.v.01 .%7CGT_PNT\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bILoai0BMjT0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You should see, among others, the first line in the corpus, which starts with:\n",
        "\n",
        "```\n",
        "the%7CGT_ART mayan%7Clem_Mayan%7CGT_ADJ%7Cwn31_Maya.n.03 image%7Clem_image%7CGT_NOU%7Cwn31_effigy.n.01 \n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NQ06M4Ow0Y-L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The file included in the github repo for this tutorial is a subset of a disambiguated tokenization for the UMBC corpus, it only contains the first 5 thousand lines of that corpus (the full corpus has about 40 million lines) as we only need it to show the steps necessary to generate embeddings.\n",
        "\n",
        "The last output, from the cell above, shows the format we are using to represent the tokenized corpus. We use white space to separate the tokens, and have URL encoded each token to avoid mixing up tokens. Since this format is hard to read, we provide a library to inspect the lines in an easy manner. Execute the following cell to display the first two lines in the corpus as a table."
      ]
    },
    {
      "metadata": {
        "id": "1NiyMpSIHDys",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "import tutorial.scripts.wntoken as wntoken\n",
        "import pandas\n",
        "\n",
        "# open the file and produce a list of python dictionaries describing the tokens\n",
        "corpus_tokens = wntoken.open_as_token_dicts(toked_corpus, max_lines=2)\n",
        "# convert the tokens into a pandas DataFrame to display in table form\n",
        "pandas.DataFrame(corpus_tokens, columns=['line', 't', 'l', 'g', 's', 'glossa'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bRkQmHPDwzfN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Example Cogito\n",
        "As a second example, analysing the original sentence:\n",
        "   \n",
        "    EXPERIMENTAL STUDY  We conducted an empirical evaluation to assess the effectiveness\n",
        "    \n",
        "using Cogito gives us     \n",
        "![Full Cogito Analysis of example sentence](https://github.com/hybridNLP2018/tutorial/blob/master/images/example-sentence-cogito.PNG?raw=1)\n",
        "\n",
        "We filter some of the words and only keep the lemmas and the syncon ids and encode them into the next sequence of disambiguated tokens:\n",
        "\n",
        "    en#86052|experimental en#2686|study en#76710|conduct en#86047|empirical en#3546|evaluation en#68903|assess \n",
        "    en#25094|effectiveness  "
      ]
    },
    {
      "metadata": {
        "id": "VT1JtXgNluKN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Vocabulary and Co-occurrence matrix\n",
        "\n",
        "Next, we need to count the co-occurrences in the disambiguated corpus. We can either:\n",
        " - use **standard swivel prep**: in this case each *<text>|<lemma>|<grammar>|<synset>* tuple will be treated as a separate token. For the example sentence from UMBC, presented above, we would then get that `mayan|lem_Mayan|GT_ADJ|wn31_Maya.n.03` has a co-occurrence count of 1 with `image|lem_image|GT_NOU|wn31_effigy.n.01`. This would result in a very large vocabulary.\n",
        " - use **joint-subtoken prep**: in this case, you can specify which individual subtoken information you want to take into account. In this notebook we will use **ls** information, hence each synset and each lemma are treated as separate entities in the vocabulary and will be represented with different embeddings. For the example sentence we would get that `lem_Mayan` has a co-occurrence count of 1 with `wn31_Maya.n.03`, `lem_image` and `wn31_effigy.n.01`. \n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "SmDALHQlluKO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_-I2CIdluKS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Standard Swivel Prep\n",
        "For the **standard swivel prep**, we can simply call `prep` using the `!python` command. In this case we have the `toked_corpus` which contains the disambiguated sequences as shown above. The output wil be a set of sharded co-occurrence submatrices as explained in the notebook for creating word vectors.\n",
        "\n",
        "We set the `shard_size` to 512 since the corpus is quite small. For larger corpora we could use the standard value of 4096."
      ]
    },
    {
      "metadata": {
        "id": "wGVS5-xbluKT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "7a65e368-67e9-4185-b0ec-160023b7ddb6"
      },
      "cell_type": "code",
      "source": [
        "!mkdir /content/umbc/\n",
        "!mkdir /content/umbc/coocs\n",
        "!mkdir /content/umbc/coocs/tlgs_wnscd_5k_standard\n",
        "coocs_path = '/content/umbc/coocs/tlgs_wnscd_5k_standard/'\n",
        "!python tutorial/scripts/swivel/prep.py --input={toked_corpus} --output_dir={coocs_path} --shard_size=512"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/umbc/’: File exists\n",
            "mkdir: cannot create directory ‘/content/umbc/coocs’: File exists\n",
            "mkdir: cannot create directory ‘/content/umbc/coocs/tlgs_wnscd_5k_standard’: File exists\n",
            "running with flags \n",
            "tutorial/scripts/swivel/prep.py:\n",
            "  --bufsz: The number of co-occurrences to buffer\n",
            "    (default: '16777216')\n",
            "    (an integer)\n",
            "  --input: The input text.\n",
            "    (default: '')\n",
            "  --max_vocab: The maximum vocabulary size\n",
            "    (default: '1048576')\n",
            "    (an integer)\n",
            "  --min_count: The minimum number of times a word should occur to be included in\n",
            "    the vocabulary\n",
            "    (default: '5')\n",
            "    (an integer)\n",
            "  --output_dir: Output directory for Swivel data\n",
            "    (default: '/tmp/swivel_data')\n",
            "  --shard_size: The size for each shard\n",
            "    (default: '4096')\n",
            "    (an integer)\n",
            "  --vocab: Vocabulary to use instead of generating one\n",
            "    (default: '')\n",
            "  --window_size: The window size\n",
            "    (default: '10')\n",
            "    (an integer)\n",
            "\n",
            "tensorflow.python.platform.app:\n",
            "  -h,--[no]help: show this help\n",
            "    (default: 'false')\n",
            "  --[no]helpfull: show full help\n",
            "    (default: 'false')\n",
            "  --[no]helpshort: show this help\n",
            "    (default: 'false')\n",
            "\n",
            "absl.flags:\n",
            "  --flagfile: Insert flag definitions from the given file into the command line.\n",
            "    (default: '')\n",
            "  --undefok: comma-separated list of flag names that it is okay to specify on\n",
            "    the command line even if the program does not define a flag with that name.\n",
            "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\n",
            "    format.\n",
            "    (default: '')\n",
            "\n",
            "vocabulary contains 8192 tokens\n",
            "\n",
            "writing shard 256/256\n",
            "Wrote vocab and sum files to /content/umbc/coocs/tlgs_wnscd_5k_standard/\n",
            "Wrote vocab and sum files to /content/umbc/coocs/tlgs_wnscd_5k_standard/\n",
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hJbMhl5lluKZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "\n",
        "    ... tensorflow flags ....\n",
        "    \n",
        "    vocabulary contains 8192 tokens\n",
        "\n",
        "    writing shard 256/256\n",
        "    Wrote vocab and sum files to /content/umbc/coocs/tlgs_wnscd_5k_standard/\n",
        "    Wrote vocab and sum files to /content/umbc/coocs/tlgs_wnscd_5k_standard/\n",
        "    done!\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9WEEg7a9068o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "68a889ca-60ce-49ef-8095-9eb3c67eaba9"
      },
      "cell_type": "code",
      "source": [
        "!head -n15  /content/umbc/coocs/tlgs_wnscd_5k_standard/row_vocab.txt"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the%7CGT_ART\n",
            "%2C%7CGT_PNT\n",
            ".%7CGT_PNT\n",
            "of%7CGT_PRE\n",
            "and%7CGT_CON\n",
            "to%7CGT_PRE\n",
            "a%7CGT_ART\n",
            "in%7CGT_PRE\n",
            "for%7CGT_PRE\n",
            "%22%7CGT_PNT\n",
            "is%7Clem_be%7CGT_VER%7Cwn31_be.v.01\n",
            "with%7CGT_PRE\n",
            "%29%7CGT_PNT\n",
            "%28%7CGT_PNT\n",
            "on%7Clem_on%7CGT_PRE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bRlYK0sZRYne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As the cells above show, applying standard prep results in a vocabulary of over 8K \"tokens\", however each token is still represented as a URL-encoded combination of the plain text, lemma, grammar type and synset (when available)."
      ]
    },
    {
      "metadata": {
        "id": "nTasIe8rluKb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Joint-subtoken Prep\n",
        "For the **joint-subtoken prep** step, we have a Java implementation that is not open-source yet (as it is still tied to proprietary code, we are working on refactoring the code so that Cogito subtokens are just a special case). However, we ***provide pre-computed co-occurrence files***.\n",
        "\n",
        "Although not open source, we describe the steps we executed to help you implement a similar pipeline. \n",
        "\n",
        "First,  we ran our implementation of subtoken prep on the corpus. Notice:\n",
        "  * we are only including lemma and synset information (i.e. we are not including plain text and grammar information). \n",
        "  * furthermore, we are filtering the corpus by\n",
        "     1. removing any tokens related to punctuation marks (PNT), auxiliary verbs (AUX) and articles (ART), since we think these do not contribute much to the semantics of words.\n",
        "     2. replacing tokens with grammar types `ENT` (entities) and `NPH` (proper names) with generic variants `grammar#ENT` and `grammar#NPH` respectively. The rationale is that, depending on the input corpus, names of people or organizations may appear a few times, but may be filtered out if they do not appear enough times. This ensures such tokens are kept in the vocabulary and contribute to the embeddings of words nearby. The main disadvantage is that we will not have some proper names in our final vocabulary.\n",
        "\n",
        "```\n",
        "java $JAVA_OPTIONS net.expertsystem.word2vec.swivel.SubtokPrep \\\n",
        "  --input C:/hybridNLP2018/tutorial/datasamples/umbc_tlgs_wnscd_5K \\\n",
        "  --output_dir C:/corpora/umbc/coocs/tlgs_wnscd_5K_ls_f/  \\\n",
        "  --expected_seq_encoding TLGS_WN \\\n",
        "  --sub_tokens \\\n",
        "  --output_subtokens \"LEMMA,SYNSET\" \\\n",
        "  --remove_tokens_with_grammar_types \"PNT,AUX,ART\"  \\\n",
        "  --generalise_tokens_with_grammar_types \"ENT,NPH\" \\\n",
        "  --shard_size 512\n",
        "```\n",
        "\n",
        "The output log looked as follows:\n",
        "\n",
        "```\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - expected_seq_encoding set to 'TLGS_WN'\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - remove_tokens_with_grammar_types set to PNT,AUX,ART\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - generalise_tokens_with_grammar_types set to ENT,NPH\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - Creating vocab for C:\\hybridNLP2018\\tutorial\\datasamples\\umbc_tlgs_wnscd_5K\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - read 5000 lines from C:\\hybridNLP2018\\tutorial\\datasamples\\umbc_tlgs_wnscd_5K\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - filtered 166152 tokens from a total of 427796 (38,839%)\n",
        "generalised 1899 tokens from a total of 427796 (0,444%)\n",
        "full vocab size 21321\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - Vocabulary contains 5632 tokens (21321 full count, 5913 appear > 5 times)\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - Flushing 1279235 co-occ pairs\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - Wrote 121 tmpShards to disk\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "PcpcqXJ_OeBk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have included the output of this process as part of the GitHub repo for the tutorial. We will unzip this folder to inspect the results:"
      ]
    },
    {
      "metadata": {
        "id": "RRUEmfptOuIr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2193
        },
        "outputId": "f4508d44-5f39-4518-d77b-0858a0b3ad29"
      },
      "cell_type": "code",
      "source": [
        "!unzip /content/tutorial/datasamples/precomp-coocs-tlgs_wnscd_5K_ls_f.zip -d /content/umbc/coocs/\n",
        "precomp_coocs_path = '/content/umbc/coocs/tlgs_wnscd_5K_ls_f'"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/tutorial/datasamples/precomp-coocs-tlgs_wnscd_5K_ls_f.zip\n",
            "   creating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/\n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/col_sums.txt  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/col_vocab.txt  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/init_vocab.txt  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/row_sums.txt  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/row_vocab.txt  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-000-000.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-000-001.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-000-002.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-000-003.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-000-004.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-000-005.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-000-006.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-000-007.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-000-008.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-000-009.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-000-010.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-001-000.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-001-001.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-001-002.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-001-003.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-001-004.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-001-005.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-001-006.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-001-007.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-001-008.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-001-009.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-001-010.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-002-000.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-002-001.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-002-002.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-002-003.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-002-004.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-002-005.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-002-006.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-002-007.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-002-008.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-002-009.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-002-010.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-003-000.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-003-001.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-003-002.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-003-003.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-003-004.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-003-005.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-003-006.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-003-007.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-003-008.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-003-009.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-003-010.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-004-000.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-004-001.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-004-002.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-004-003.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-004-004.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-004-005.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-004-006.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-004-007.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-004-008.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-004-009.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-004-010.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-005-000.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-005-001.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-005-002.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-005-003.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-005-004.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-005-005.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-005-006.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-005-007.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-005-008.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-005-009.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-005-010.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-006-000.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-006-001.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-006-002.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-006-003.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-006-004.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-006-005.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-006-006.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-006-007.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-006-008.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-006-009.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-006-010.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-007-000.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-007-001.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-007-002.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-007-003.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-007-004.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-007-005.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-007-006.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-007-007.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-007-008.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-007-009.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-007-010.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-008-000.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-008-001.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-008-002.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-008-003.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-008-004.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-008-005.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-008-006.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-008-007.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-008-008.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-008-009.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-008-010.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-009-000.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-009-001.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-009-002.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-009-003.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-009-004.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-009-005.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-009-006.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-009-007.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-009-008.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-009-009.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-009-010.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-010-000.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-010-001.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-010-002.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-010-003.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-010-004.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-010-005.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-010-006.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-010-007.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-010-008.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-010-009.pb  \n",
            "  inflating: /content/umbc/coocs/tlgs_wnscd_5K_ls_f/shard-010-010.pb  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WwTFnSPTQBxl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The previous cell extracts the pre-computed co-occurrence shards and defines a variable `precomp_coocs_path` that points to the folder where these shards are stored.\n",
        "\n",
        "Next, we print the first 10 elements of the vocabulary to see the format that we are using to represent the lemmas and synsets:"
      ]
    },
    {
      "metadata": {
        "id": "uf6Vas-TP6h2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a20b680c-1e2c-4151-f564-7c6df3b2623e"
      },
      "cell_type": "code",
      "source": [
        "!head -n10 {precomp_coocs_path}/row_vocab.txt"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lem_be\n",
            "wn31_be.v.01\n",
            "lem_that\n",
            "lem_this\n",
            "lem_on\n",
            "lem_by\n",
            "lem_information\n",
            "lem_as\n",
            "lem_use\n",
            "lem_from\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OvK8-CaBSOOO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As the output above shows, the vocabulary we get with `subtoken prep` is smaller (5.6K elements instead of over 8K) and it contains individual lemmas and synsets (it also contains *special* elements grammar#ENT and grammar#NPH, as described above).\n",
        "\n",
        "**More importantly**, the co-occurrence counts take into account the fact that certain lemmas co-occur more frequently with certain other lemmas and synsets, which should be taken into account when learning embedding representations."
      ]
    },
    {
      "metadata": {
        "id": "YE0VZd9lluKf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Learn embeddings from co-occurrence matrix\n",
        "\n",
        "With the sharded co-occurrence matrices created in the previous section it is now possible to learn embeddings by calling the `swivel.py` script. This launches a tensorflow application based on various parameters (most of which are self-explanatory) :\n",
        "\n",
        " - `input_base_path`: the folder with the co-occurrence matrix (protobuf files with the sparse matrix) generated above.\n",
        " - `submatrix_` rows and columns need to be the same size as the `shard_size` used in the `prep` step.\n",
        " - `num_epochs` the number of times to go through the input data (all the co-occurrences in the shards). We have found that for large corpora, the learning algorithm converges after a few epochs, while for smaller corpora you need a larger number of epochs. \n",
        " \n",
        " Execute the following cell to generate embeddings for the pre-computed co-occurrences."
      ]
    },
    {
      "metadata": {
        "id": "Adtnl3HWluKf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8724
        },
        "outputId": "eff09276-b7fc-4a34-fdcc-f43ead233825"
      },
      "cell_type": "code",
      "source": [
        "vec_path = '/content/umbc/vec/tlgs_wnscd_5k_ls_f'\n",
        "!python /content/tutorial/scripts/swivel/swivel.py --input_base_path={precomp_coocs_path} \\\n",
        "    --output_base_path={vec_path} \\\n",
        "    --num_epochs=40 --dim=150 \\\n",
        "    --submatrix_rows=512 --submatrix_cols=512"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:187: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /content/tutorial/scripts/swivel/swivel.py:495: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Saving checkpoint to path /content/umbc/vec/tlgs_wnscd_5k_ls_f/model.ckpt\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "INFO:tensorflow:global_step/sec: 0\n",
            "INFO:tensorflow:Recording summary at step 0.\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n",
            "INFO:tensorflow:local_step=10 global_step=10 loss=82.3, 0.2% complete\n",
            "INFO:tensorflow:local_step=20 global_step=20 loss=78.6, 0.4% complete\n",
            "INFO:tensorflow:local_step=30 global_step=30 loss=76.6, 0.6% complete\n",
            "INFO:tensorflow:local_step=40 global_step=40 loss=74.5, 0.8% complete\n",
            "INFO:tensorflow:local_step=50 global_step=50 loss=72.0, 1.0% complete\n",
            "INFO:tensorflow:local_step=60 global_step=60 loss=67.0, 1.2% complete\n",
            "INFO:tensorflow:local_step=70 global_step=70 loss=60.1, 1.4% complete\n",
            "INFO:tensorflow:local_step=80 global_step=80 loss=58.1, 1.7% complete\n",
            "INFO:tensorflow:local_step=90 global_step=90 loss=55.1, 1.9% complete\n",
            "INFO:tensorflow:local_step=100 global_step=100 loss=60.1, 2.1% complete\n",
            "INFO:tensorflow:local_step=110 global_step=110 loss=72.0, 2.3% complete\n",
            "INFO:tensorflow:local_step=120 global_step=120 loss=51.3, 2.5% complete\n",
            "INFO:tensorflow:local_step=130 global_step=130 loss=52.8, 2.7% complete\n",
            "INFO:tensorflow:local_step=140 global_step=140 loss=43.6, 2.9% complete\n",
            "INFO:tensorflow:local_step=150 global_step=150 loss=48.1, 3.1% complete\n",
            "INFO:tensorflow:local_step=160 global_step=160 loss=43.0, 3.3% complete\n",
            "INFO:tensorflow:local_step=170 global_step=170 loss=52.9, 3.5% complete\n",
            "INFO:tensorflow:local_step=180 global_step=180 loss=49.9, 3.7% complete\n",
            "INFO:tensorflow:local_step=190 global_step=190 loss=40.9, 3.9% complete\n",
            "INFO:tensorflow:local_step=200 global_step=200 loss=43.1, 4.1% complete\n",
            "INFO:tensorflow:local_step=210 global_step=210 loss=47.4, 4.3% complete\n",
            "INFO:tensorflow:local_step=220 global_step=220 loss=48.0, 4.5% complete\n",
            "INFO:tensorflow:local_step=230 global_step=230 loss=49.7, 4.8% complete\n",
            "INFO:tensorflow:local_step=240 global_step=240 loss=46.5, 5.0% complete\n",
            "INFO:tensorflow:local_step=250 global_step=250 loss=44.4, 5.2% complete\n",
            "INFO:tensorflow:local_step=260 global_step=260 loss=45.3, 5.4% complete\n",
            "INFO:tensorflow:local_step=270 global_step=270 loss=44.1, 5.6% complete\n",
            "INFO:tensorflow:local_step=280 global_step=280 loss=33.6, 5.8% complete\n",
            "INFO:tensorflow:local_step=290 global_step=290 loss=34.7, 6.0% complete\n",
            "INFO:tensorflow:local_step=300 global_step=300 loss=43.2, 6.2% complete\n",
            "INFO:tensorflow:local_step=310 global_step=310 loss=41.3, 6.4% complete\n",
            "INFO:tensorflow:local_step=320 global_step=320 loss=38.5, 6.6% complete\n",
            "INFO:tensorflow:local_step=330 global_step=330 loss=36.6, 6.8% complete\n",
            "INFO:tensorflow:local_step=340 global_step=340 loss=40.5, 7.0% complete\n",
            "INFO:tensorflow:local_step=350 global_step=350 loss=49.6, 7.2% complete\n",
            "INFO:tensorflow:local_step=360 global_step=360 loss=38.0, 7.4% complete\n",
            "INFO:tensorflow:local_step=370 global_step=370 loss=32.1, 7.6% complete\n",
            "INFO:tensorflow:local_step=380 global_step=380 loss=39.2, 7.9% complete\n",
            "INFO:tensorflow:local_step=390 global_step=390 loss=45.8, 8.1% complete\n",
            "INFO:tensorflow:local_step=400 global_step=400 loss=40.4, 8.3% complete\n",
            "INFO:tensorflow:local_step=410 global_step=410 loss=34.5, 8.5% complete\n",
            "INFO:tensorflow:local_step=420 global_step=420 loss=45.4, 8.7% complete\n",
            "INFO:tensorflow:local_step=430 global_step=430 loss=39.9, 8.9% complete\n",
            "INFO:tensorflow:local_step=440 global_step=440 loss=45.7, 9.1% complete\n",
            "INFO:tensorflow:local_step=450 global_step=450 loss=39.8, 9.3% complete\n",
            "INFO:tensorflow:local_step=460 global_step=460 loss=43.5, 9.5% complete\n",
            "INFO:tensorflow:local_step=470 global_step=470 loss=46.6, 9.7% complete\n",
            "INFO:tensorflow:local_step=480 global_step=480 loss=42.7, 9.9% complete\n",
            "INFO:tensorflow:local_step=490 global_step=490 loss=75.4, 10.1% complete\n",
            "INFO:tensorflow:local_step=500 global_step=500 loss=37.5, 10.3% complete\n",
            "INFO:tensorflow:local_step=510 global_step=510 loss=41.2, 10.5% complete\n",
            "INFO:tensorflow:local_step=520 global_step=520 loss=37.9, 10.7% complete\n",
            "INFO:tensorflow:local_step=530 global_step=530 loss=44.6, 11.0% complete\n",
            "INFO:tensorflow:local_step=540 global_step=540 loss=46.7, 11.2% complete\n",
            "INFO:tensorflow:local_step=550 global_step=550 loss=37.3, 11.4% complete\n",
            "INFO:tensorflow:local_step=560 global_step=560 loss=38.1, 11.6% complete\n",
            "INFO:tensorflow:local_step=570 global_step=570 loss=42.4, 11.8% complete\n",
            "INFO:tensorflow:local_step=580 global_step=580 loss=148.4, 12.0% complete\n",
            "INFO:tensorflow:local_step=590 global_step=590 loss=41.2, 12.2% complete\n",
            "INFO:tensorflow:local_step=600 global_step=600 loss=37.0, 12.4% complete\n",
            "INFO:tensorflow:local_step=610 global_step=610 loss=44.1, 12.6% complete\n",
            "INFO:tensorflow:local_step=620 global_step=620 loss=39.4, 12.8% complete\n",
            "INFO:tensorflow:local_step=630 global_step=630 loss=33.0, 13.0% complete\n",
            "INFO:tensorflow:local_step=640 global_step=640 loss=44.3, 13.2% complete\n",
            "INFO:tensorflow:local_step=650 global_step=650 loss=43.0, 13.4% complete\n",
            "INFO:tensorflow:local_step=660 global_step=660 loss=147.3, 13.6% complete\n",
            "INFO:tensorflow:local_step=670 global_step=670 loss=40.5, 13.8% complete\n",
            "INFO:tensorflow:local_step=680 global_step=680 loss=45.4, 14.0% complete\n",
            "INFO:tensorflow:local_step=690 global_step=690 loss=36.2, 14.3% complete\n",
            "INFO:tensorflow:local_step=700 global_step=700 loss=38.0, 14.5% complete\n",
            "INFO:tensorflow:local_step=710 global_step=710 loss=43.1, 14.7% complete\n",
            "INFO:tensorflow:local_step=720 global_step=720 loss=37.2, 14.9% complete\n",
            "INFO:tensorflow:local_step=730 global_step=730 loss=38.8, 15.1% complete\n",
            "INFO:tensorflow:local_step=740 global_step=740 loss=38.6, 15.3% complete\n",
            "INFO:tensorflow:local_step=750 global_step=750 loss=38.3, 15.5% complete\n",
            "INFO:tensorflow:local_step=760 global_step=760 loss=43.9, 15.7% complete\n",
            "INFO:tensorflow:local_step=770 global_step=770 loss=43.8, 15.9% complete\n",
            "INFO:tensorflow:local_step=780 global_step=780 loss=147.2, 16.1% complete\n",
            "INFO:tensorflow:local_step=790 global_step=790 loss=40.8, 16.3% complete\n",
            "INFO:tensorflow:local_step=800 global_step=800 loss=183.2, 16.5% complete\n",
            "INFO:tensorflow:local_step=810 global_step=810 loss=41.6, 16.7% complete\n",
            "INFO:tensorflow:local_step=820 global_step=820 loss=45.1, 16.9% complete\n",
            "INFO:tensorflow:local_step=830 global_step=830 loss=32.9, 17.1% complete\n",
            "INFO:tensorflow:local_step=840 global_step=840 loss=41.4, 17.4% complete\n",
            "INFO:tensorflow:local_step=850 global_step=850 loss=38.2, 17.6% complete\n",
            "INFO:tensorflow:local_step=860 global_step=860 loss=35.2, 17.8% complete\n",
            "INFO:tensorflow:local_step=870 global_step=870 loss=39.4, 18.0% complete\n",
            "INFO:tensorflow:local_step=880 global_step=880 loss=35.8, 18.2% complete\n",
            "INFO:tensorflow:local_step=890 global_step=890 loss=145.6, 18.4% complete\n",
            "INFO:tensorflow:local_step=900 global_step=900 loss=38.5, 18.6% complete\n",
            "INFO:tensorflow:local_step=910 global_step=910 loss=40.9, 18.8% complete\n",
            "INFO:tensorflow:local_step=920 global_step=920 loss=41.1, 19.0% complete\n",
            "INFO:tensorflow:local_step=930 global_step=930 loss=41.3, 19.2% complete\n",
            "INFO:tensorflow:local_step=940 global_step=940 loss=39.9, 19.4% complete\n",
            "INFO:tensorflow:local_step=950 global_step=950 loss=40.6, 19.6% complete\n",
            "INFO:tensorflow:local_step=960 global_step=960 loss=45.3, 19.8% complete\n",
            "INFO:tensorflow:local_step=970 global_step=970 loss=39.4, 20.0% complete\n",
            "INFO:tensorflow:local_step=980 global_step=980 loss=40.9, 20.2% complete\n",
            "INFO:tensorflow:local_step=990 global_step=990 loss=38.5, 20.5% complete\n",
            "INFO:tensorflow:local_step=1000 global_step=1000 loss=43.8, 20.7% complete\n",
            "INFO:tensorflow:local_step=1010 global_step=1010 loss=35.6, 20.9% complete\n",
            "INFO:tensorflow:local_step=1020 global_step=1020 loss=45.3, 21.1% complete\n",
            "INFO:tensorflow:local_step=1030 global_step=1030 loss=39.2, 21.3% complete\n",
            "INFO:tensorflow:local_step=1040 global_step=1040 loss=44.3, 21.5% complete\n",
            "INFO:tensorflow:local_step=1050 global_step=1050 loss=40.2, 21.7% complete\n",
            "INFO:tensorflow:local_step=1060 global_step=1060 loss=43.2, 21.9% complete\n",
            "INFO:tensorflow:local_step=1070 global_step=1070 loss=42.3, 22.1% complete\n",
            "INFO:tensorflow:local_step=1080 global_step=1080 loss=41.0, 22.3% complete\n",
            "INFO:tensorflow:local_step=1090 global_step=1090 loss=39.5, 22.5% complete\n",
            "INFO:tensorflow:local_step=1100 global_step=1100 loss=34.9, 22.7% complete\n",
            "INFO:tensorflow:local_step=1110 global_step=1110 loss=35.0, 22.9% complete\n",
            "INFO:tensorflow:local_step=1120 global_step=1120 loss=32.8, 23.1% complete\n",
            "INFO:tensorflow:local_step=1130 global_step=1130 loss=42.4, 23.3% complete\n",
            "INFO:tensorflow:local_step=1140 global_step=1140 loss=38.8, 23.6% complete\n",
            "INFO:tensorflow:local_step=1150 global_step=1150 loss=42.2, 23.8% complete\n",
            "INFO:tensorflow:local_step=1160 global_step=1160 loss=44.5, 24.0% complete\n",
            "INFO:tensorflow:local_step=1170 global_step=1170 loss=39.3, 24.2% complete\n",
            "INFO:tensorflow:local_step=1180 global_step=1180 loss=46.8, 24.4% complete\n",
            "INFO:tensorflow:local_step=1190 global_step=1190 loss=41.8, 24.6% complete\n",
            "INFO:tensorflow:local_step=1200 global_step=1200 loss=39.9, 24.8% complete\n",
            "INFO:tensorflow:local_step=1210 global_step=1210 loss=38.8, 25.0% complete\n",
            "INFO:tensorflow:local_step=1220 global_step=1220 loss=33.8, 25.2% complete\n",
            "INFO:tensorflow:local_step=1230 global_step=1230 loss=38.6, 25.4% complete\n",
            "INFO:tensorflow:local_step=1240 global_step=1240 loss=37.3, 25.6% complete\n",
            "INFO:tensorflow:local_step=1250 global_step=1250 loss=40.1, 25.8% complete\n",
            "INFO:tensorflow:local_step=1260 global_step=1260 loss=37.8, 26.0% complete\n",
            "INFO:tensorflow:local_step=1270 global_step=1270 loss=44.4, 26.2% complete\n",
            "INFO:tensorflow:local_step=1280 global_step=1280 loss=41.6, 26.4% complete\n",
            "INFO:tensorflow:local_step=1290 global_step=1290 loss=43.8, 26.7% complete\n",
            "INFO:tensorflow:local_step=1300 global_step=1300 loss=39.1, 26.9% complete\n",
            "INFO:tensorflow:local_step=1310 global_step=1310 loss=41.6, 27.1% complete\n",
            "INFO:tensorflow:local_step=1320 global_step=1320 loss=36.5, 27.3% complete\n",
            "INFO:tensorflow:local_step=1330 global_step=1330 loss=34.9, 27.5% complete\n",
            "INFO:tensorflow:local_step=1340 global_step=1340 loss=41.1, 27.7% complete\n",
            "INFO:tensorflow:local_step=1350 global_step=1350 loss=40.2, 27.9% complete\n",
            "INFO:tensorflow:local_step=1360 global_step=1360 loss=39.4, 28.1% complete\n",
            "INFO:tensorflow:local_step=1370 global_step=1370 loss=141.4, 28.3% complete\n",
            "INFO:tensorflow:local_step=1380 global_step=1380 loss=42.3, 28.5% complete\n",
            "INFO:tensorflow:local_step=1390 global_step=1390 loss=41.8, 28.7% complete\n",
            "INFO:tensorflow:local_step=1400 global_step=1400 loss=38.9, 28.9% complete\n",
            "INFO:tensorflow:local_step=1410 global_step=1410 loss=44.8, 29.1% complete\n",
            "INFO:tensorflow:local_step=1420 global_step=1420 loss=39.9, 29.3% complete\n",
            "INFO:tensorflow:local_step=1430 global_step=1430 loss=42.8, 29.5% complete\n",
            "INFO:tensorflow:local_step=1440 global_step=1440 loss=42.0, 29.8% complete\n",
            "INFO:tensorflow:local_step=1450 global_step=1450 loss=40.2, 30.0% complete\n",
            "INFO:tensorflow:local_step=1460 global_step=1460 loss=38.8, 30.2% complete\n",
            "INFO:tensorflow:local_step=1470 global_step=1470 loss=44.2, 30.4% complete\n",
            "INFO:tensorflow:local_step=1480 global_step=1480 loss=37.4, 30.6% complete\n",
            "INFO:tensorflow:local_step=1490 global_step=1490 loss=46.2, 30.8% complete\n",
            "INFO:tensorflow:local_step=1500 global_step=1500 loss=45.2, 31.0% complete\n",
            "INFO:tensorflow:local_step=1510 global_step=1510 loss=39.4, 31.2% complete\n",
            "INFO:tensorflow:local_step=1520 global_step=1520 loss=40.6, 31.4% complete\n",
            "INFO:tensorflow:local_step=1530 global_step=1530 loss=163.9, 31.6% complete\n",
            "INFO:tensorflow:local_step=1540 global_step=1540 loss=42.9, 31.8% complete\n",
            "INFO:tensorflow:local_step=1550 global_step=1550 loss=38.3, 32.0% complete\n",
            "INFO:tensorflow:local_step=1560 global_step=1560 loss=45.0, 32.2% complete\n",
            "INFO:tensorflow:local_step=1570 global_step=1570 loss=44.1, 32.4% complete\n",
            "INFO:tensorflow:local_step=1580 global_step=1580 loss=34.1, 32.6% complete\n",
            "INFO:tensorflow:local_step=1590 global_step=1590 loss=41.7, 32.9% complete\n",
            "INFO:tensorflow:local_step=1600 global_step=1600 loss=41.7, 33.1% complete\n",
            "INFO:tensorflow:local_step=1610 global_step=1610 loss=34.9, 33.3% complete\n",
            "INFO:tensorflow:local_step=1620 global_step=1620 loss=38.4, 33.5% complete\n",
            "INFO:tensorflow:local_step=1630 global_step=1630 loss=31.6, 33.7% complete\n",
            "INFO:tensorflow:local_step=1640 global_step=1640 loss=173.3, 33.9% complete\n",
            "INFO:tensorflow:local_step=1650 global_step=1650 loss=151.5, 34.1% complete\n",
            "INFO:tensorflow:local_step=1660 global_step=1660 loss=38.4, 34.3% complete\n",
            "INFO:tensorflow:local_step=1670 global_step=1670 loss=42.4, 34.5% complete\n",
            "INFO:tensorflow:local_step=1680 global_step=1680 loss=37.9, 34.7% complete\n",
            "INFO:tensorflow:local_step=1690 global_step=1690 loss=34.4, 34.9% complete\n",
            "INFO:tensorflow:local_step=1700 global_step=1700 loss=36.2, 35.1% complete\n",
            "INFO:tensorflow:local_step=1710 global_step=1710 loss=38.8, 35.3% complete\n",
            "INFO:tensorflow:local_step=1720 global_step=1720 loss=37.0, 35.5% complete\n",
            "INFO:tensorflow:local_step=1730 global_step=1730 loss=44.2, 35.7% complete\n",
            "INFO:tensorflow:local_step=1740 global_step=1740 loss=43.8, 36.0% complete\n",
            "INFO:tensorflow:local_step=1750 global_step=1750 loss=36.9, 36.2% complete\n",
            "INFO:tensorflow:local_step=1760 global_step=1760 loss=44.7, 36.4% complete\n",
            "INFO:tensorflow:local_step=1770 global_step=1770 loss=44.3, 36.6% complete\n",
            "INFO:tensorflow:local_step=1780 global_step=1780 loss=38.3, 36.8% complete\n",
            "INFO:tensorflow:local_step=1790 global_step=1790 loss=42.6, 37.0% complete\n",
            "INFO:tensorflow:local_step=1800 global_step=1800 loss=41.7, 37.2% complete\n",
            "INFO:tensorflow:local_step=1810 global_step=1810 loss=39.6, 37.4% complete\n",
            "INFO:tensorflow:local_step=1820 global_step=1820 loss=36.5, 37.6% complete\n",
            "INFO:tensorflow:local_step=1830 global_step=1830 loss=31.1, 37.8% complete\n",
            "INFO:tensorflow:local_step=1840 global_step=1840 loss=35.3, 38.0% complete\n",
            "INFO:tensorflow:local_step=1850 global_step=1850 loss=40.8, 38.2% complete\n",
            "INFO:tensorflow:local_step=1860 global_step=1860 loss=35.7, 38.4% complete\n",
            "INFO:tensorflow:local_step=1870 global_step=1870 loss=41.2, 38.6% complete\n",
            "INFO:tensorflow:local_step=1880 global_step=1880 loss=184.6, 38.8% complete\n",
            "INFO:tensorflow:local_step=1890 global_step=1890 loss=42.1, 39.0% complete\n",
            "INFO:tensorflow:local_step=1900 global_step=1900 loss=41.9, 39.3% complete\n",
            "INFO:tensorflow:local_step=1910 global_step=1910 loss=39.1, 39.5% complete\n",
            "INFO:tensorflow:local_step=1920 global_step=1920 loss=33.4, 39.7% complete\n",
            "INFO:tensorflow:local_step=1930 global_step=1930 loss=43.2, 39.9% complete\n",
            "INFO:tensorflow:local_step=1940 global_step=1940 loss=36.6, 40.1% complete\n",
            "INFO:tensorflow:local_step=1950 global_step=1950 loss=43.0, 40.3% complete\n",
            "INFO:tensorflow:local_step=1960 global_step=1960 loss=39.1, 40.5% complete\n",
            "INFO:tensorflow:local_step=1970 global_step=1970 loss=39.0, 40.7% complete\n",
            "INFO:tensorflow:local_step=1980 global_step=1980 loss=44.0, 40.9% complete\n",
            "INFO:tensorflow:local_step=1990 global_step=1990 loss=34.4, 41.1% complete\n",
            "INFO:tensorflow:local_step=2000 global_step=2000 loss=36.4, 41.3% complete\n",
            "INFO:tensorflow:local_step=2010 global_step=2010 loss=43.9, 41.5% complete\n",
            "INFO:tensorflow:local_step=2020 global_step=2020 loss=44.7, 41.7% complete\n",
            "INFO:tensorflow:local_step=2030 global_step=2030 loss=41.3, 41.9% complete\n",
            "INFO:tensorflow:local_step=2040 global_step=2040 loss=197.0, 42.1% complete\n",
            "INFO:tensorflow:local_step=2050 global_step=2050 loss=38.8, 42.4% complete\n",
            "INFO:tensorflow:local_step=2060 global_step=2060 loss=35.0, 42.6% complete\n",
            "INFO:tensorflow:local_step=2070 global_step=2070 loss=36.8, 42.8% complete\n",
            "INFO:tensorflow:local_step=2080 global_step=2080 loss=37.1, 43.0% complete\n",
            "INFO:tensorflow:local_step=2090 global_step=2090 loss=35.8, 43.2% complete\n",
            "INFO:tensorflow:local_step=2100 global_step=2100 loss=257.0, 43.4% complete\n",
            "INFO:tensorflow:local_step=2110 global_step=2110 loss=42.3, 43.6% complete\n",
            "INFO:tensorflow:local_step=2120 global_step=2120 loss=42.0, 43.8% complete\n",
            "INFO:tensorflow:local_step=2130 global_step=2130 loss=38.7, 44.0% complete\n",
            "INFO:tensorflow:local_step=2140 global_step=2140 loss=267.0, 44.2% complete\n",
            "INFO:tensorflow:local_step=2150 global_step=2150 loss=37.4, 44.4% complete\n",
            "INFO:tensorflow:local_step=2160 global_step=2160 loss=42.8, 44.6% complete\n",
            "INFO:tensorflow:local_step=2170 global_step=2170 loss=39.4, 44.8% complete\n",
            "INFO:tensorflow:local_step=2180 global_step=2180 loss=42.1, 45.0% complete\n",
            "INFO:tensorflow:local_step=2190 global_step=2190 loss=33.7, 45.2% complete\n",
            "INFO:tensorflow:local_step=2200 global_step=2200 loss=38.2, 45.5% complete\n",
            "INFO:tensorflow:local_step=2210 global_step=2210 loss=43.1, 45.7% complete\n",
            "INFO:tensorflow:local_step=2220 global_step=2220 loss=41.9, 45.9% complete\n",
            "INFO:tensorflow:local_step=2230 global_step=2230 loss=43.9, 46.1% complete\n",
            "INFO:tensorflow:local_step=2240 global_step=2240 loss=40.5, 46.3% complete\n",
            "INFO:tensorflow:local_step=2250 global_step=2250 loss=34.7, 46.5% complete\n",
            "INFO:tensorflow:local_step=2260 global_step=2260 loss=40.0, 46.7% complete\n",
            "INFO:tensorflow:local_step=2270 global_step=2270 loss=37.0, 46.9% complete\n",
            "INFO:tensorflow:local_step=2280 global_step=2280 loss=42.6, 47.1% complete\n",
            "INFO:tensorflow:local_step=2290 global_step=2290 loss=36.4, 47.3% complete\n",
            "INFO:tensorflow:local_step=2300 global_step=2300 loss=38.2, 47.5% complete\n",
            "INFO:tensorflow:local_step=2310 global_step=2310 loss=36.2, 47.7% complete\n",
            "INFO:tensorflow:local_step=2320 global_step=2320 loss=28.8, 47.9% complete\n",
            "INFO:tensorflow:local_step=2330 global_step=2330 loss=39.5, 48.1% complete\n",
            "INFO:tensorflow:local_step=2340 global_step=2340 loss=41.0, 48.3% complete\n",
            "INFO:tensorflow:local_step=2350 global_step=2350 loss=31.6, 48.6% complete\n",
            "INFO:tensorflow:local_step=2360 global_step=2360 loss=36.1, 48.8% complete\n",
            "INFO:tensorflow:local_step=2370 global_step=2370 loss=32.1, 49.0% complete\n",
            "INFO:tensorflow:local_step=2380 global_step=2380 loss=47.6, 49.2% complete\n",
            "INFO:tensorflow:local_step=2390 global_step=2390 loss=44.3, 49.4% complete\n",
            "INFO:tensorflow:local_step=2400 global_step=2400 loss=36.9, 49.6% complete\n",
            "INFO:tensorflow:local_step=2410 global_step=2410 loss=33.0, 49.8% complete\n",
            "INFO:tensorflow:local_step=2420 global_step=2420 loss=36.6, 50.0% complete\n",
            "INFO:tensorflow:local_step=2430 global_step=2430 loss=42.2, 50.2% complete\n",
            "INFO:tensorflow:local_step=2440 global_step=2440 loss=36.8, 50.4% complete\n",
            "INFO:tensorflow:local_step=2450 global_step=2450 loss=39.5, 50.6% complete\n",
            "INFO:tensorflow:local_step=2460 global_step=2460 loss=38.0, 50.8% complete\n",
            "INFO:tensorflow:local_step=2470 global_step=2470 loss=48.5, 51.0% complete\n",
            "INFO:tensorflow:local_step=2480 global_step=2480 loss=37.2, 51.2% complete\n",
            "INFO:tensorflow:local_step=2490 global_step=2490 loss=39.1, 51.4% complete\n",
            "INFO:tensorflow:local_step=2500 global_step=2500 loss=31.6, 51.7% complete\n",
            "INFO:tensorflow:local_step=2510 global_step=2510 loss=46.9, 51.9% complete\n",
            "INFO:tensorflow:local_step=2520 global_step=2520 loss=41.7, 52.1% complete\n",
            "INFO:tensorflow:local_step=2530 global_step=2530 loss=36.5, 52.3% complete\n",
            "INFO:tensorflow:local_step=2540 global_step=2540 loss=45.3, 52.5% complete\n",
            "INFO:tensorflow:local_step=2550 global_step=2550 loss=145.8, 52.7% complete\n",
            "INFO:tensorflow:local_step=2560 global_step=2560 loss=38.6, 52.9% complete\n",
            "INFO:tensorflow:local_step=2570 global_step=2570 loss=33.8, 53.1% complete\n",
            "INFO:tensorflow:local_step=2580 global_step=2580 loss=35.0, 53.3% complete\n",
            "INFO:tensorflow:local_step=2590 global_step=2590 loss=42.0, 53.5% complete\n",
            "INFO:tensorflow:local_step=2600 global_step=2600 loss=35.6, 53.7% complete\n",
            "INFO:tensorflow:local_step=2610 global_step=2610 loss=39.8, 53.9% complete\n",
            "INFO:tensorflow:local_step=2620 global_step=2620 loss=38.6, 54.1% complete\n",
            "INFO:tensorflow:local_step=2630 global_step=2630 loss=259.1, 54.3% complete\n",
            "INFO:tensorflow:local_step=2640 global_step=2640 loss=38.9, 54.5% complete\n",
            "INFO:tensorflow:local_step=2650 global_step=2650 loss=39.8, 54.8% complete\n",
            "INFO:tensorflow:local_step=2660 global_step=2660 loss=45.6, 55.0% complete\n",
            "INFO:tensorflow:local_step=2670 global_step=2670 loss=35.9, 55.2% complete\n",
            "INFO:tensorflow:local_step=2680 global_step=2680 loss=166.7, 55.4% complete\n",
            "INFO:tensorflow:local_step=2690 global_step=2690 loss=35.8, 55.6% complete\n",
            "INFO:tensorflow:local_step=2700 global_step=2700 loss=31.5, 55.8% complete\n",
            "INFO:tensorflow:local_step=2710 global_step=2710 loss=36.7, 56.0% complete\n",
            "INFO:tensorflow:local_step=2720 global_step=2720 loss=43.5, 56.2% complete\n",
            "INFO:tensorflow:local_step=2730 global_step=2730 loss=38.2, 56.4% complete\n",
            "INFO:tensorflow:local_step=2740 global_step=2740 loss=40.9, 56.6% complete\n",
            "INFO:tensorflow:local_step=2750 global_step=2750 loss=36.3, 56.8% complete\n",
            "INFO:tensorflow:local_step=2760 global_step=2760 loss=39.8, 57.0% complete\n",
            "INFO:tensorflow:local_step=2770 global_step=2770 loss=39.8, 57.2% complete\n",
            "INFO:tensorflow:local_step=2780 global_step=2780 loss=42.7, 57.4% complete\n",
            "INFO:tensorflow:local_step=2790 global_step=2790 loss=37.9, 57.6% complete\n",
            "INFO:tensorflow:local_step=2800 global_step=2800 loss=37.5, 57.9% complete\n",
            "INFO:tensorflow:local_step=2810 global_step=2810 loss=42.0, 58.1% complete\n",
            "INFO:tensorflow:local_step=2820 global_step=2820 loss=38.8, 58.3% complete\n",
            "INFO:tensorflow:local_step=2830 global_step=2830 loss=44.9, 58.5% complete\n",
            "INFO:tensorflow:local_step=2840 global_step=2840 loss=42.1, 58.7% complete\n",
            "INFO:tensorflow:local_step=2850 global_step=2850 loss=44.7, 58.9% complete\n",
            "INFO:tensorflow:local_step=2860 global_step=2860 loss=35.1, 59.1% complete\n",
            "INFO:tensorflow:local_step=2870 global_step=2870 loss=43.2, 59.3% complete\n",
            "INFO:tensorflow:local_step=2880 global_step=2880 loss=39.1, 59.5% complete\n",
            "INFO:tensorflow:local_step=2890 global_step=2890 loss=48.5, 59.7% complete\n",
            "INFO:tensorflow:local_step=2900 global_step=2900 loss=35.8, 59.9% complete\n",
            "INFO:tensorflow:local_step=2910 global_step=2910 loss=28.5, 60.1% complete\n",
            "INFO:tensorflow:local_step=2920 global_step=2920 loss=33.8, 60.3% complete\n",
            "INFO:tensorflow:local_step=2930 global_step=2930 loss=43.6, 60.5% complete\n",
            "INFO:tensorflow:local_step=2940 global_step=2940 loss=42.3, 60.7% complete\n",
            "INFO:tensorflow:local_step=2950 global_step=2950 loss=40.9, 61.0% complete\n",
            "INFO:tensorflow:local_step=2960 global_step=2960 loss=182.8, 61.2% complete\n",
            "INFO:tensorflow:local_step=2970 global_step=2970 loss=39.9, 61.4% complete\n",
            "INFO:tensorflow:local_step=2980 global_step=2980 loss=35.1, 61.6% complete\n",
            "INFO:tensorflow:local_step=2990 global_step=2990 loss=39.4, 61.8% complete\n",
            "INFO:tensorflow:local_step=3000 global_step=3000 loss=43.0, 62.0% complete\n",
            "INFO:tensorflow:local_step=3010 global_step=3010 loss=38.7, 62.2% complete\n",
            "INFO:tensorflow:local_step=3020 global_step=3020 loss=40.8, 62.4% complete\n",
            "INFO:tensorflow:local_step=3030 global_step=3030 loss=39.5, 62.6% complete\n",
            "INFO:tensorflow:local_step=3040 global_step=3040 loss=39.2, 62.8% complete\n",
            "INFO:tensorflow:local_step=3050 global_step=3050 loss=33.1, 63.0% complete\n",
            "INFO:tensorflow:local_step=3060 global_step=3060 loss=40.7, 63.2% complete\n",
            "INFO:tensorflow:local_step=3070 global_step=3070 loss=40.9, 63.4% complete\n",
            "INFO:tensorflow:local_step=3080 global_step=3080 loss=31.4, 63.6% complete\n",
            "INFO:tensorflow:local_step=3090 global_step=3090 loss=40.0, 63.8% complete\n",
            "INFO:tensorflow:local_step=3100 global_step=3100 loss=42.4, 64.0% complete\n",
            "INFO:tensorflow:local_step=3110 global_step=3110 loss=32.6, 64.3% complete\n",
            "INFO:tensorflow:local_step=3120 global_step=3120 loss=35.2, 64.5% complete\n",
            "INFO:tensorflow:local_step=3130 global_step=3130 loss=39.7, 64.7% complete\n",
            "INFO:tensorflow:local_step=3140 global_step=3140 loss=40.5, 64.9% complete\n",
            "INFO:tensorflow:local_step=3150 global_step=3150 loss=39.6, 65.1% complete\n",
            "INFO:tensorflow:local_step=3160 global_step=3160 loss=40.6, 65.3% complete\n",
            "INFO:tensorflow:local_step=3170 global_step=3170 loss=35.8, 65.5% complete\n",
            "INFO:tensorflow:global_step/sec: 52.9212\n",
            "INFO:tensorflow:Recording summary at step 3174.\n",
            "INFO:tensorflow:local_step=3180 global_step=3180 loss=187.3, 65.7% complete\n",
            "INFO:tensorflow:local_step=3190 global_step=3190 loss=42.4, 65.9% complete\n",
            "INFO:tensorflow:local_step=3200 global_step=3200 loss=38.3, 66.1% complete\n",
            "INFO:tensorflow:local_step=3210 global_step=3210 loss=189.1, 66.3% complete\n",
            "INFO:tensorflow:local_step=3220 global_step=3220 loss=43.6, 66.5% complete\n",
            "INFO:tensorflow:local_step=3230 global_step=3230 loss=37.4, 66.7% complete\n",
            "INFO:tensorflow:local_step=3240 global_step=3240 loss=40.5, 66.9% complete\n",
            "INFO:tensorflow:local_step=3250 global_step=3250 loss=39.7, 67.1% complete\n",
            "INFO:tensorflow:local_step=3260 global_step=3260 loss=46.3, 67.4% complete\n",
            "INFO:tensorflow:local_step=3270 global_step=3270 loss=40.2, 67.6% complete\n",
            "INFO:tensorflow:local_step=3280 global_step=3280 loss=41.3, 67.8% complete\n",
            "INFO:tensorflow:local_step=3290 global_step=3290 loss=174.1, 68.0% complete\n",
            "INFO:tensorflow:local_step=3300 global_step=3300 loss=38.0, 68.2% complete\n",
            "INFO:tensorflow:local_step=3310 global_step=3310 loss=37.0, 68.4% complete\n",
            "INFO:tensorflow:local_step=3320 global_step=3320 loss=36.8, 68.6% complete\n",
            "INFO:tensorflow:local_step=3330 global_step=3330 loss=42.3, 68.8% complete\n",
            "INFO:tensorflow:local_step=3340 global_step=3340 loss=40.9, 69.0% complete\n",
            "INFO:tensorflow:local_step=3350 global_step=3350 loss=37.5, 69.2% complete\n",
            "INFO:tensorflow:local_step=3360 global_step=3360 loss=39.2, 69.4% complete\n",
            "INFO:tensorflow:local_step=3370 global_step=3370 loss=41.6, 69.6% complete\n",
            "INFO:tensorflow:local_step=3380 global_step=3380 loss=40.1, 69.8% complete\n",
            "INFO:tensorflow:local_step=3390 global_step=3390 loss=39.1, 70.0% complete\n",
            "INFO:tensorflow:local_step=3400 global_step=3400 loss=37.2, 70.2% complete\n",
            "INFO:tensorflow:local_step=3410 global_step=3410 loss=31.4, 70.5% complete\n",
            "INFO:tensorflow:local_step=3420 global_step=3420 loss=38.9, 70.7% complete\n",
            "INFO:tensorflow:local_step=3430 global_step=3430 loss=230.5, 70.9% complete\n",
            "INFO:tensorflow:local_step=3440 global_step=3440 loss=37.3, 71.1% complete\n",
            "INFO:tensorflow:local_step=3450 global_step=3450 loss=165.5, 71.3% complete\n",
            "INFO:tensorflow:local_step=3460 global_step=3460 loss=40.5, 71.5% complete\n",
            "INFO:tensorflow:local_step=3470 global_step=3470 loss=34.9, 71.7% complete\n",
            "INFO:tensorflow:local_step=3480 global_step=3480 loss=38.1, 71.9% complete\n",
            "INFO:tensorflow:local_step=3490 global_step=3490 loss=205.7, 72.1% complete\n",
            "INFO:tensorflow:local_step=3500 global_step=3500 loss=46.1, 72.3% complete\n",
            "INFO:tensorflow:local_step=3510 global_step=3510 loss=41.8, 72.5% complete\n",
            "INFO:tensorflow:local_step=3520 global_step=3520 loss=41.8, 72.7% complete\n",
            "INFO:tensorflow:local_step=3530 global_step=3530 loss=43.2, 72.9% complete\n",
            "INFO:tensorflow:local_step=3540 global_step=3540 loss=40.5, 73.1% complete\n",
            "INFO:tensorflow:local_step=3550 global_step=3550 loss=40.6, 73.3% complete\n",
            "INFO:tensorflow:local_step=3560 global_step=3560 loss=41.6, 73.6% complete\n",
            "INFO:tensorflow:local_step=3570 global_step=3570 loss=40.3, 73.8% complete\n",
            "INFO:tensorflow:local_step=3580 global_step=3580 loss=39.4, 74.0% complete\n",
            "INFO:tensorflow:local_step=3590 global_step=3590 loss=39.2, 74.2% complete\n",
            "INFO:tensorflow:local_step=3600 global_step=3600 loss=36.3, 74.4% complete\n",
            "INFO:tensorflow:local_step=3610 global_step=3610 loss=36.8, 74.6% complete\n",
            "INFO:tensorflow:local_step=3620 global_step=3620 loss=43.2, 74.8% complete\n",
            "INFO:tensorflow:local_step=3630 global_step=3630 loss=35.7, 75.0% complete\n",
            "INFO:tensorflow:local_step=3640 global_step=3640 loss=40.8, 75.2% complete\n",
            "INFO:tensorflow:local_step=3650 global_step=3650 loss=39.6, 75.4% complete\n",
            "INFO:tensorflow:local_step=3660 global_step=3660 loss=38.1, 75.6% complete\n",
            "INFO:tensorflow:local_step=3670 global_step=3670 loss=178.6, 75.8% complete\n",
            "INFO:tensorflow:local_step=3680 global_step=3680 loss=35.4, 76.0% complete\n",
            "INFO:tensorflow:local_step=3690 global_step=3690 loss=43.3, 76.2% complete\n",
            "INFO:tensorflow:local_step=3700 global_step=3700 loss=39.0, 76.4% complete\n",
            "INFO:tensorflow:local_step=3710 global_step=3710 loss=40.5, 76.7% complete\n",
            "INFO:tensorflow:local_step=3720 global_step=3720 loss=40.6, 76.9% complete\n",
            "INFO:tensorflow:local_step=3730 global_step=3730 loss=38.7, 77.1% complete\n",
            "INFO:tensorflow:local_step=3740 global_step=3740 loss=38.4, 77.3% complete\n",
            "INFO:tensorflow:local_step=3750 global_step=3750 loss=36.4, 77.5% complete\n",
            "INFO:tensorflow:local_step=3760 global_step=3760 loss=39.9, 77.7% complete\n",
            "INFO:tensorflow:local_step=3770 global_step=3770 loss=40.5, 77.9% complete\n",
            "INFO:tensorflow:local_step=3780 global_step=3780 loss=43.7, 78.1% complete\n",
            "INFO:tensorflow:local_step=3790 global_step=3790 loss=44.6, 78.3% complete\n",
            "INFO:tensorflow:local_step=3800 global_step=3800 loss=42.9, 78.5% complete\n",
            "INFO:tensorflow:local_step=3810 global_step=3810 loss=301.0, 78.7% complete\n",
            "INFO:tensorflow:local_step=3820 global_step=3820 loss=39.9, 78.9% complete\n",
            "INFO:tensorflow:local_step=3830 global_step=3830 loss=41.9, 79.1% complete\n",
            "INFO:tensorflow:local_step=3840 global_step=3840 loss=37.1, 79.3% complete\n",
            "INFO:tensorflow:local_step=3850 global_step=3850 loss=36.6, 79.5% complete\n",
            "INFO:tensorflow:local_step=3860 global_step=3860 loss=40.5, 79.8% complete\n",
            "INFO:tensorflow:local_step=3870 global_step=3870 loss=172.2, 80.0% complete\n",
            "INFO:tensorflow:local_step=3880 global_step=3880 loss=32.5, 80.2% complete\n",
            "INFO:tensorflow:local_step=3890 global_step=3890 loss=39.5, 80.4% complete\n",
            "INFO:tensorflow:local_step=3900 global_step=3900 loss=41.0, 80.6% complete\n",
            "INFO:tensorflow:local_step=3910 global_step=3910 loss=35.9, 80.8% complete\n",
            "INFO:tensorflow:local_step=3920 global_step=3920 loss=145.7, 81.0% complete\n",
            "INFO:tensorflow:local_step=3930 global_step=3930 loss=36.6, 81.2% complete\n",
            "INFO:tensorflow:local_step=3940 global_step=3940 loss=40.5, 81.4% complete\n",
            "INFO:tensorflow:local_step=3950 global_step=3950 loss=44.3, 81.6% complete\n",
            "INFO:tensorflow:local_step=3960 global_step=3960 loss=40.1, 81.8% complete\n",
            "INFO:tensorflow:local_step=3970 global_step=3970 loss=36.0, 82.0% complete\n",
            "INFO:tensorflow:local_step=3980 global_step=3980 loss=41.0, 82.2% complete\n",
            "INFO:tensorflow:local_step=3990 global_step=3990 loss=44.0, 82.4% complete\n",
            "INFO:tensorflow:local_step=4000 global_step=4000 loss=40.5, 82.6% complete\n",
            "INFO:tensorflow:local_step=4010 global_step=4010 loss=36.4, 82.9% complete\n",
            "INFO:tensorflow:local_step=4020 global_step=4020 loss=47.4, 83.1% complete\n",
            "INFO:tensorflow:local_step=4030 global_step=4030 loss=31.6, 83.3% complete\n",
            "INFO:tensorflow:local_step=4040 global_step=4040 loss=39.9, 83.5% complete\n",
            "INFO:tensorflow:local_step=4050 global_step=4050 loss=38.9, 83.7% complete\n",
            "INFO:tensorflow:local_step=4060 global_step=4060 loss=41.3, 83.9% complete\n",
            "INFO:tensorflow:local_step=4070 global_step=4070 loss=42.5, 84.1% complete\n",
            "INFO:tensorflow:local_step=4080 global_step=4080 loss=34.5, 84.3% complete\n",
            "INFO:tensorflow:local_step=4090 global_step=4090 loss=33.9, 84.5% complete\n",
            "INFO:tensorflow:local_step=4100 global_step=4100 loss=35.4, 84.7% complete\n",
            "INFO:tensorflow:local_step=4110 global_step=4110 loss=45.3, 84.9% complete\n",
            "INFO:tensorflow:local_step=4120 global_step=4120 loss=35.1, 85.1% complete\n",
            "INFO:tensorflow:local_step=4130 global_step=4130 loss=38.7, 85.3% complete\n",
            "INFO:tensorflow:local_step=4140 global_step=4140 loss=43.3, 85.5% complete\n",
            "INFO:tensorflow:local_step=4150 global_step=4150 loss=41.2, 85.7% complete\n",
            "INFO:tensorflow:local_step=4160 global_step=4160 loss=34.0, 86.0% complete\n",
            "INFO:tensorflow:local_step=4170 global_step=4170 loss=44.2, 86.2% complete\n",
            "INFO:tensorflow:local_step=4180 global_step=4180 loss=40.7, 86.4% complete\n",
            "INFO:tensorflow:local_step=4190 global_step=4190 loss=35.7, 86.6% complete\n",
            "INFO:tensorflow:local_step=4200 global_step=4200 loss=37.6, 86.8% complete\n",
            "INFO:tensorflow:local_step=4210 global_step=4210 loss=41.1, 87.0% complete\n",
            "INFO:tensorflow:local_step=4220 global_step=4220 loss=38.7, 87.2% complete\n",
            "INFO:tensorflow:local_step=4230 global_step=4230 loss=257.0, 87.4% complete\n",
            "INFO:tensorflow:local_step=4240 global_step=4240 loss=32.4, 87.6% complete\n",
            "INFO:tensorflow:local_step=4250 global_step=4250 loss=34.2, 87.8% complete\n",
            "INFO:tensorflow:local_step=4260 global_step=4260 loss=33.1, 88.0% complete\n",
            "INFO:tensorflow:local_step=4270 global_step=4270 loss=35.4, 88.2% complete\n",
            "INFO:tensorflow:local_step=4280 global_step=4280 loss=41.7, 88.4% complete\n",
            "INFO:tensorflow:local_step=4290 global_step=4290 loss=37.4, 88.6% complete\n",
            "INFO:tensorflow:local_step=4300 global_step=4300 loss=38.8, 88.8% complete\n",
            "INFO:tensorflow:local_step=4310 global_step=4310 loss=44.9, 89.0% complete\n",
            "INFO:tensorflow:local_step=4320 global_step=4320 loss=36.0, 89.3% complete\n",
            "INFO:tensorflow:local_step=4330 global_step=4330 loss=40.9, 89.5% complete\n",
            "INFO:tensorflow:local_step=4340 global_step=4340 loss=45.1, 89.7% complete\n",
            "INFO:tensorflow:local_step=4350 global_step=4350 loss=39.6, 89.9% complete\n",
            "INFO:tensorflow:local_step=4360 global_step=4360 loss=42.1, 90.1% complete\n",
            "INFO:tensorflow:local_step=4370 global_step=4370 loss=29.8, 90.3% complete\n",
            "INFO:tensorflow:local_step=4380 global_step=4380 loss=36.2, 90.5% complete\n",
            "INFO:tensorflow:local_step=4390 global_step=4390 loss=42.4, 90.7% complete\n",
            "INFO:tensorflow:local_step=4400 global_step=4400 loss=43.4, 90.9% complete\n",
            "INFO:tensorflow:local_step=4410 global_step=4410 loss=170.7, 91.1% complete\n",
            "INFO:tensorflow:local_step=4420 global_step=4420 loss=39.1, 91.3% complete\n",
            "INFO:tensorflow:local_step=4430 global_step=4430 loss=40.0, 91.5% complete\n",
            "INFO:tensorflow:local_step=4440 global_step=4440 loss=37.5, 91.7% complete\n",
            "INFO:tensorflow:local_step=4450 global_step=4450 loss=42.0, 91.9% complete\n",
            "INFO:tensorflow:local_step=4460 global_step=4460 loss=201.4, 92.1% complete\n",
            "INFO:tensorflow:local_step=4470 global_step=4470 loss=42.1, 92.4% complete\n",
            "INFO:tensorflow:local_step=4480 global_step=4480 loss=33.0, 92.6% complete\n",
            "INFO:tensorflow:local_step=4490 global_step=4490 loss=42.4, 92.8% complete\n",
            "INFO:tensorflow:local_step=4500 global_step=4500 loss=38.4, 93.0% complete\n",
            "INFO:tensorflow:local_step=4510 global_step=4510 loss=40.5, 93.2% complete\n",
            "INFO:tensorflow:local_step=4520 global_step=4520 loss=42.1, 93.4% complete\n",
            "INFO:tensorflow:local_step=4530 global_step=4530 loss=34.5, 93.6% complete\n",
            "INFO:tensorflow:local_step=4540 global_step=4540 loss=36.5, 93.8% complete\n",
            "INFO:tensorflow:local_step=4550 global_step=4550 loss=41.1, 94.0% complete\n",
            "INFO:tensorflow:local_step=4560 global_step=4560 loss=47.8, 94.2% complete\n",
            "INFO:tensorflow:local_step=4570 global_step=4570 loss=40.1, 94.4% complete\n",
            "INFO:tensorflow:local_step=4580 global_step=4580 loss=45.2, 94.6% complete\n",
            "INFO:tensorflow:local_step=4590 global_step=4590 loss=40.3, 94.8% complete\n",
            "INFO:tensorflow:local_step=4600 global_step=4600 loss=38.6, 95.0% complete\n",
            "INFO:tensorflow:local_step=4610 global_step=4610 loss=39.4, 95.2% complete\n",
            "INFO:tensorflow:local_step=4620 global_step=4620 loss=46.0, 95.5% complete\n",
            "INFO:tensorflow:local_step=4630 global_step=4630 loss=34.5, 95.7% complete\n",
            "INFO:tensorflow:local_step=4640 global_step=4640 loss=43.1, 95.9% complete\n",
            "INFO:tensorflow:local_step=4650 global_step=4650 loss=32.6, 96.1% complete\n",
            "INFO:tensorflow:local_step=4660 global_step=4660 loss=44.8, 96.3% complete\n",
            "INFO:tensorflow:local_step=4670 global_step=4670 loss=35.9, 96.5% complete\n",
            "INFO:tensorflow:local_step=4680 global_step=4680 loss=37.0, 96.7% complete\n",
            "INFO:tensorflow:local_step=4690 global_step=4690 loss=36.2, 96.9% complete\n",
            "INFO:tensorflow:local_step=4700 global_step=4700 loss=41.9, 97.1% complete\n",
            "INFO:tensorflow:local_step=4710 global_step=4710 loss=39.3, 97.3% complete\n",
            "INFO:tensorflow:local_step=4720 global_step=4720 loss=137.0, 97.5% complete\n",
            "INFO:tensorflow:local_step=4730 global_step=4730 loss=145.2, 97.7% complete\n",
            "INFO:tensorflow:local_step=4740 global_step=4740 loss=35.1, 97.9% complete\n",
            "INFO:tensorflow:local_step=4750 global_step=4750 loss=41.9, 98.1% complete\n",
            "INFO:tensorflow:local_step=4760 global_step=4760 loss=229.0, 98.3% complete\n",
            "INFO:tensorflow:local_step=4770 global_step=4770 loss=38.2, 98.6% complete\n",
            "INFO:tensorflow:local_step=4780 global_step=4780 loss=34.7, 98.8% complete\n",
            "INFO:tensorflow:local_step=4790 global_step=4790 loss=44.6, 99.0% complete\n",
            "INFO:tensorflow:local_step=4800 global_step=4800 loss=41.4, 99.2% complete\n",
            "INFO:tensorflow:local_step=4810 global_step=4810 loss=36.3, 99.4% complete\n",
            "INFO:tensorflow:local_step=4820 global_step=4820 loss=40.2, 99.6% complete\n",
            "INFO:tensorflow:local_step=4830 global_step=4830 loss=41.4, 99.8% complete\n",
            "INFO:tensorflow:local_step=4840 global_step=4840 loss=31.1, 100.0% complete\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5JItR-RVluKl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This will take a few minutes, depending on your machine.\n",
        "The result is a list of files in the specified output folder, including:\n",
        " - the tensorflow graph, which defines the architecture of the model being trained\n",
        " - checkpoints of the model (intermediate snapshots of the weights)\n",
        " - `tsv` files for the final state of the column and row embeddings."
      ]
    },
    {
      "metadata": {
        "id": "FORz_vtGluKm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c65304a9-13fd-40a0-a363-d2f66c0d78d7"
      },
      "cell_type": "code",
      "source": [
        "%ls {vec_path}"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\n",
            "col_embedding.tsv\n",
            "events.out.tfevents.1538661737.5b2dbcb47226\n",
            "graph.pbtxt\n",
            "model.ckpt-0.data-00000-of-00001\n",
            "model.ckpt-0.index\n",
            "model.ckpt-0.meta\n",
            "model.ckpt-4840.data-00000-of-00001\n",
            "model.ckpt-4840.index\n",
            "model.ckpt-4840.meta\n",
            "row_embedding.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IrMmRcouluKq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert `tsv` files to `bin` file\n",
        "\n",
        "As we've seen in previous notebooks, the `tsv` files are easy to inspect, but they take too much space and they are slow to load since we need to convert the different values to floats and pack them as vectors. Swivel offers a utility to convert the `tsv` files into a `bin`ary format. At the same time it combines the column and row embeddings into a single space (it simply adds the two vectors for each word in the vocabulary)."
      ]
    },
    {
      "metadata": {
        "id": "IBtZG82kluKr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b7db4321-925b-479f-8453-55163a5e28f5"
      },
      "cell_type": "code",
      "source": [
        "!python /content/tutorial/scripts/swivel/text2bin.py --vocab={precomp_coocs_path}/row_vocab.txt --output={vec_path}/vecs.bin \\\n",
        "        {vec_path}/row_embedding.tsv \\\n",
        "        {vec_path}/col_embedding.tsv"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "executing text2bin\n",
            "merging files ['/content/umbc/vec/tlgs_wnscd_5k_ls_f/row_embedding.tsv', '/content/umbc/vec/tlgs_wnscd_5k_ls_f/col_embedding.tsv'] into output bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZXQOe0SWluKw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This adds the `vocab.txt` and `vecs.bin` to the folder with the vectors:"
      ]
    },
    {
      "metadata": {
        "id": "t1jV7uoGluKx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "0183e4b5-fa64-4cc5-a789-b14ebe43fa95"
      },
      "cell_type": "code",
      "source": [
        "%ls {vec_path}"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\n",
            "col_embedding.tsv\n",
            "events.out.tfevents.1538661737.5b2dbcb47226\n",
            "graph.pbtxt\n",
            "model.ckpt-0.data-00000-of-00001\n",
            "model.ckpt-0.index\n",
            "model.ckpt-0.meta\n",
            "model.ckpt-4840.data-00000-of-00001\n",
            "model.ckpt-4840.index\n",
            "model.ckpt-4840.meta\n",
            "row_embedding.tsv\n",
            "vecs.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yL26TO9BluK4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inspect the embeddings\n",
        "\n",
        "As in previous notebooks, we can now use Swivel to inspect the vectors using the `Vecs` class. It accepts a `vocab_file` and a file for the binary serialization of the vectors (`vecs.bin`)."
      ]
    },
    {
      "metadata": {
        "id": "f3QA6U6nluK5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tutorial.scripts.swivel import vecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h3cXtjfYluK7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "...and we can load existing vectors. Here we load some pre-computed embeddings, but feel free to use the embeddings you computed by following the steps above (although, due to random initialization of weight during the training step, your results may be different)."
      ]
    },
    {
      "metadata": {
        "id": "e7VejLMMluK8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c1c6f803-c9e1-4597-87a1-23adbe0aa38c"
      },
      "cell_type": "code",
      "source": [
        "vectors = vecs.Vecs(precomp_coocs_path + '/row_vocab.txt', \n",
        "            vec_path + '/vecs.bin')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Opening vector with expected size 5632 from file /content/umbc/coocs/tlgs_wnscd_5K_ls_f/row_vocab.txt\n",
            "vocab size 5632 (unique 5632)\n",
            "read rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LJCKmEpAluLB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, let's define a basic method for printing the `k` nearest neighbors for a given word:"
      ]
    },
    {
      "metadata": {
        "id": "LcjM72nRluLD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def k_neighbors(word, k=10):\n",
        "    res = vectors.neighbors(word)\n",
        "    if not res:\n",
        "        print('%s is not in the vocabulary, try e.g. %s' % (word, vecs.random_word_in_vocab()))\n",
        "    else:\n",
        "        for word, sim in res[:10]:\n",
        "            print('%0.4f: %s' % (sim, word))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V8dR-NxRluLH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And let's use the method on a few lemmas and synsets in the vocabulary:"
      ]
    },
    {
      "metadata": {
        "id": "AffhuN8SluLH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "06e0c24c-4e73-4243-bed4-5d685c2a2a0c"
      },
      "cell_type": "code",
      "source": [
        "k_neighbors('lem_California')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0000: lem_California\n",
            "0.5068: lem_University of California\n",
            "0.3764: wn31_recognize.v.08\n",
            "0.3148: lem_comprise\n",
            "0.2934: lem_battle\n",
            "0.2912: lem_deployment\n",
            "0.2840: wn31_map.v.04\n",
            "0.2792: grammar#ENT\n",
            "0.2624: wn31_publication.n.04\n",
            "0.2565: lem_Caribbean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6IE6BC63j9KW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "17915183-d34e-42e1-8eab-6e0119ad0dbd"
      },
      "cell_type": "code",
      "source": [
        "k_neighbors('lem_semantic')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0000: lem_semantic\n",
            "0.3592: wn31_map.v.01\n",
            "0.3559: lem_procedural\n",
            "0.3517: lem_dictionary\n",
            "0.3468: lem_relationship\n",
            "0.3334: lem_object-oriented\n",
            "0.3277: lem_similarity\n",
            "0.3247: wn31_common.a.01\n",
            "0.3139: wn31_similarity.n.01\n",
            "0.3134: lem_header\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ffWNLUVHkJfr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b3ff1bcb-a89b-4af5-8e56-0fac5ef85f6c"
      },
      "cell_type": "code",
      "source": [
        "k_neighbors('lem_conference')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0000: lem_conference\n",
            "0.6992: wn31_conference.n.01\n",
            "0.4336: wn31_session.n.01\n",
            "0.4307: lem_proceedings\n",
            "0.4257: lem_annual\n",
            "0.4160: wn31_conference.n.03\n",
            "0.3906: wn31_seminar.n.01\n",
            "0.3890: lem_workshop\n",
            "0.3809: lem_seminar\n",
            "0.3803: lem_hold\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_K8HU-LDkjW-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c55be5f9-e3e1-4a7b-f69b-bacf6d40d278"
      },
      "cell_type": "code",
      "source": [
        "k_neighbors('wn31_conference.n.01')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0000: wn31_conference.n.01\n",
            "0.6992: lem_conference\n",
            "0.4521: wn31_seminar.n.01\n",
            "0.4392: wn31_annual.a.01\n",
            "0.4329: lem_annual\n",
            "0.4231: wn31_external.a.03\n",
            "0.4072: lem_seminar\n",
            "0.3822: lem_proceedings\n",
            "0.3613: wn31_meeting.n.01\n",
            "0.3539: wn31_session.n.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0_Q1p9DbluLM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that using the Vecsigrafo approach gets us very different results than when using standard swivel (notebook 01):\n",
        " * the results now include concepts (synsets), besides just words. Without further information, this makes interpreting the results harder since we now only have the concept id, but we can search for these concepts in the underlying KG (WordNet in this case) to explore the semantic network and get further information.\n",
        " \n",
        "Of course, results may not be very good, since these have been derived from a very small corpus (5K lines from UMBC). In the excercise below, we encourage you to download and inspect pre-computed embeddings based on the full UMBC corpus."
      ]
    },
    {
      "metadata": {
        "id": "PCG_FvU6luLM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "66ab4fc7-4a3c-41c9-e423-58dc82cdf347"
      },
      "cell_type": "code",
      "source": [
        "k_neighbors('lem_semantic web')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0000: lem_semantic web\n",
            "0.3511: wn31_technology.n.01\n",
            "0.3413: lem_machine learning\n",
            "0.3242: wn31_employment.n.01\n",
            "0.3156: lem_emergence\n",
            "0.2912: wn31_model.n.04\n",
            "0.2868: lem_incorporation\n",
            "0.2858: lem_infrastructure\n",
            "0.2830: lem_technology\n",
            "0.2819: lem_educationally\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7rKAc6T_luLm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c3706fbe-d69c-40d3-e608-942d271fcf1d"
      },
      "cell_type": "code",
      "source": [
        "k_neighbors('lem_ontology')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0000: lem_ontology\n",
            "0.3664: lem_eye\n",
            "0.3107: lem_extend\n",
            "0.3028: wn31_function.n.01\n",
            "0.2987: lem_rdf\n",
            "0.2955: lem_concepts\n",
            "0.2910: wn31_existent.a.01\n",
            "0.2886: lem_mapping\n",
            "0.2807: lem_unified\n",
            "0.2806: lem_relationship\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KAZGM9PcluLr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion and Exercises\n",
        "\n",
        "In this notebook we generated a vecsigrafo based on a disambiguated corpus. The resulting embedding space combines concept ids and lemmas.\n",
        "\n",
        "We have seen that the resulting space:\n",
        " 1. may be harder to inspect due to the potentially opaque concept ids\n",
        " 2. clearly different than standard swivel embeddings\n",
        " \n",
        "The question is: are the resulting embeddings *better*? \n",
        "\n",
        "To get an answer, in the next notebook, we will look at **evaluation methods for embeddings**."
      ]
    },
    {
      "metadata": {
        "id": "oOgdJ4BBgS-E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Explore full precomputed embeddings\n",
        "\n",
        "We have also pre-computed embeddings for the full UMBC corpus. The provided `tar.gz` file is about 1.1GB, hence downloading it may take several minutes."
      ]
    },
    {
      "metadata": {
        "id": "6bxeWC28gRzy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b82b9d3b-4545-475c-d231-679525d6b443"
      },
      "cell_type": "code",
      "source": [
        "full_precomp_url = 'https://esdrive.expertsystem.com/public/file/AYBZoTKJ80yibBVv-z84iA/vecsigrafo_umbc_tlgs_ls_f_6e_160d_row_embedding.tar.gz'\n",
        "full_precomp_targz = '/content/umbc/vec/tlgs_wnscd_ls_f_6e_160d_row_embedding.tar.gz'\n",
        "!wget {full_precomp_url} -O {full_precomp_targz}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-04 15:05:57--  https://esdrive.expertsystem.com/public/file/AYBZoTKJ80yibBVv-z84iA/vecsigrafo_umbc_tlgs_ls_f_6e_160d_row_embedding.tar.gz\n",
            "Resolving esdrive.expertsystem.com (esdrive.expertsystem.com)... 195.78.200.194\n",
            "Connecting to esdrive.expertsystem.com (esdrive.expertsystem.com)|195.78.200.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 Ok\n",
            "Length: 1166454112 (1.1G) [application/x-gzip]\n",
            "Saving to: ‘/content/umbc/vec/tlgs_wnscd_ls_f_6e_160d_row_embedding.tar.gz’\n",
            "\n",
            "    /content/umbc/v  50%[=========>          ] 567.18M  1.59MB/s    eta 5m 43s "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xg1Og_PVnMZ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!tar -xzf {full_precomp_targz} -C /content/umbc/vec/\n",
        "full_precomp_vec_path = '/content/umbc/vec/vecsi_tlgs_wnscd_ls_f_6e_160d'"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}