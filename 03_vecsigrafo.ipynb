{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_vecsigrafo.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "o5OAyd4nluKL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generate a Vecsigrafo using Swivel\n",
        "\n",
        "\n",
        "In this notebook we show how to generate a Vecsigrafo based on a subset of the [UMBC corpus](https://ebiquity.umbc.edu/resource/html/id/351/UMBC-webbase-corpus).\n",
        "\n",
        "We follow the procedure described in [Towards a Vecsigrafo: Portable Semantics in Knowledge-based Text Analytics](https://pdfs.semanticscholar.org/b0d6/197940d8f1a5fa0d7474bd9a94bd9e44a0ee.pdf) and depicted in the following figure:\n",
        "\n",
        "![Generic Vecsigrafo Creation](https://github.com/hybridNLP2018/tutorial/blob/master/images/generic-vecsigrafo-creation.png?raw=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xLwTh2Qhs5Lt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenization and Word Sense Disambiguation\n",
        "\n",
        "The main difference with standard swivel is that:\n",
        " - we use word-sense disambiguation on the text as a pre-processing step (Swivel simply uses white-space tokenization)\n",
        " - each 'token' in the resulting sequences is composed of a lemma and an optional concept identifier.\n",
        " \n",
        "### Disambiguators\n",
        "If we are going to apply WSD, we will need some disambiguator strategy. Unfortunately, there are not a lot of open-source high-performance disambiguators available. At [Expert System](https://www.expertsystem.com/), we have a [state-of-the-art disambiguator](https://www.expertsystem.com/products/cogito-cognitive-technology/semantic-technology/disambiguation/) that assings **syncon**s (our version of synsets) to lemmas in the text. \n",
        "\n",
        "Since Expert System's disambiguator and semantic KG are proprietary, in this notebook we will be mostly using WordNet (although we may present some results and examples based on Expert System's results). We have implemented a lightweight disambiguation strategy, proposed by [Mancini, M., Camacho-Collados, J., Iacobacci, I., & Navigli, R. (2017). Embedding Words and Senses Together via Joint Knowledge-Enhanced Training. CoNLL.](http://arxiv.org/abs/1612.02703), which has allowed us to produce disambiguated corpora based on WordNet 3.1.\n",
        "\n",
        "To be able to inspect the disambiguated corpus, let's make sure we have access to WordNet in our environment by executing the following cell.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7PGZ7dicIUAI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "wn.synset('Maya.n.02')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bhQ3aE4oKnIp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenizations\n",
        "When applying a disambiguator, the tokens are no longer (groups of) words. Each token can contain different types of information, we generally keep the following token information:\n",
        "  * `t`: text, the original text (possibly normalised, i.e. lower-cased)\n",
        "  * `l`: lemma, the lemma form of the word\n",
        "  * `g`: grammar: the grammar type\n",
        "  * `s`: syncon (or synset) identifier\n",
        "  \n",
        "### Example WordNet\n",
        "\n",
        "We have included a small sample of our disambiguated UMBC corpus as part of our [GitHub tutorial repo](https://github.com/HybridNLP2018/tutorial). Execute the following cell to clone the repo, unzip the sample corpus and print the first line of the corpus"
      ]
    },
    {
      "metadata": {
        "id": "b0s_9qVzw5bd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/HybridNLP2018/tutorial.git\n",
        "%cd /content/tutorial/datasamples/\n",
        "!unzip umbc_tlgs_wnscd_5K.zip\n",
        "toked_corpus = '/content/tutorial/datasamples/umbc_tlgs_wnscd_5K'\n",
        "!head -n1 {toked_corpus}\n",
        "%cd /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bILoai0BMjT0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You should see, among others, the first line in the corpus, which starts with:\n",
        "\n",
        "```\n",
        "the%7CGT_ART mayan%7Clem_Mayan%7CGT_ADJ%7Cwn31_Maya.n.03 image%7Clem_image%7CGT_NOU%7Cwn31_effigy.n.01 \n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NQ06M4Ow0Y-L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The file included in the github repo for this tutorial is a subset of a disambiguated tokenization for the UMBC corpus, it only contains the first 5 thousand lines of that corpus (the full corpus has about 40 million lines) as we only need it to show the steps necessary to generate embeddings.\n",
        "\n",
        "The last output, from the cell above, shows the format we are using to represent the tokenized corpus. We use white space to separate the tokens, and have URL encoded each token to avoid mixing up tokens. Since this format is hard to read, we provide a library to inspect the lines in an easy manner. Execute the following cell to display the first two lines in the corpus as a table."
      ]
    },
    {
      "metadata": {
        "id": "1NiyMpSIHDys",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "import tutorial.scripts.wntoken as wntoken\n",
        "import pandas\n",
        "\n",
        "# open the file and produce a list of python dictionaries describing the tokens\n",
        "corpus_tokens = wntoken.open_as_token_dicts(toked_corpus, max_lines=2)\n",
        "# convert the tokens into a pandas DataFrame to display in table form\n",
        "pandas.DataFrame(corpus_tokens, columns=['line', 't', 'l', 'g', 's', 'glossa'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bRkQmHPDwzfN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Example Cogito\n",
        "As a second example, analysing the original sentence:\n",
        "   \n",
        "    EXPERIMENTAL STUDY  We conducted an empirical evaluation to assess the effectiveness\n",
        "    \n",
        "using Cogito gives us     \n",
        "![Full Cogito Analysis of example sentence](https://github.com/hybridNLP2018/tutorial/blob/master/images/example-sentence-cogito.PNG?raw=1)\n",
        "\n",
        "We filter some of the words and only keep the lemmas and the syncon ids and encode them into the next sequence of disambiguated tokens:\n",
        "\n",
        "    en#86052|experimental en#2686|study en#76710|conduct en#86047|empirical en#3546|evaluation en#68903|assess \n",
        "    en#25094|effectiveness  "
      ]
    },
    {
      "metadata": {
        "id": "VT1JtXgNluKN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Vocabulary and Co-occurrence matrix\n",
        "\n",
        "Next, we need to count the co-occurrences in the disambiguated corpus. We can either:\n",
        " - use **standard swivel prep**: in this case each *<text>|<lemma>|<grammar>|<synset>* tuple will be treated as a separate token. For the example sentence from UMBC, presented above, we would then get that `mayan|lem_Mayan|GT_ADJ|wn31_Maya.n.03` has a co-occurrence count of 1 with `image|lem_image|GT_NOU|wn31_effigy.n.01`. This would result in a very large vocabulary.\n",
        " - use **joint-subtoken prep**: in this case, you can specify which individual subtoken information you want to take into account. In this notebook we will use **ls** information, hence each synset and each lemma are treated as separate entities in the vocabulary and will be represented with different embeddings. For the example sentence we would get that `lem_Mayan` has a co-occurrence count of 1 with `wn31_Maya.n.03`, `lem_image` and `wn31_effigy.n.01`. \n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "SmDALHQlluKO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_-I2CIdluKS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Standard Swivel Prep\n",
        "For the **standard swivel prep**, we can simply call `prep` using the `!python` command. In this case we have the `toked_corpus` which contains the disambiguated sequences as shown above. The output wil be a set of sharded co-occurrence submatrices as explained in the notebook for creating word vectors.\n",
        "\n",
        "We set the `shard_size` to 512 since the corpus is quite small. For larger corpora we could use the standard value of 4096."
      ]
    },
    {
      "metadata": {
        "id": "wGVS5-xbluKT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir /content/umbc/\n",
        "!mkdir /content/umbc/coocs\n",
        "!mkdir /content/umbc/coocs/tlgs_wnscd_5k_standard\n",
        "coocs_path = '/content/umbc/coocs/tlgs_wnscd_5k_standard/'\n",
        "!python tutorial/scripts/swivel/prep.py --input={toked_corpus} --output_dir={coocs_path} --shard_size=512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hJbMhl5lluKZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "\n",
        "    ... tensorflow flags ....\n",
        "    \n",
        "    vocabulary contains 8192 tokens\n",
        "\n",
        "    writing shard 256/256\n",
        "    Wrote vocab and sum files to /content/umbc/coocs/tlgs_wnscd_5k_standard/\n",
        "    Wrote vocab and sum files to /content/umbc/coocs/tlgs_wnscd_5k_standard/\n",
        "    done!\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9WEEg7a9068o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head -n15  /content/umbc/coocs/tlgs_wnscd_5k_standard/row_vocab.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bRlYK0sZRYne",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As the cells above show, applying standard prep results in a vocabulary of over 8K \"tokens\", however each token is still represented as a URL-encoded combination of the plain text, lemma, grammar type and synset (when available)."
      ]
    },
    {
      "metadata": {
        "id": "nTasIe8rluKb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Joint-subtoken Prep\n",
        "For the **joint-subtoken prep** step, we have a Java implementation that is not open-source yet (as it is still tied to proprietary code, we are working on refactoring the code so that Cogito subtokens are just a special case). However, we ***provide pre-computed co-occurrence files***.\n",
        "\n",
        "Although not open source, we describe the steps we executed to help you implement a similar pipeline. \n",
        "\n",
        "First,  we ran our implementation of subtoken prep on the corpus. Notice:\n",
        "  * we are only including lemma and synset information (i.e. we are not including plain text and grammar information). \n",
        "  * furthermore, we are filtering the corpus by\n",
        "     1. removing any tokens related to punctuation marks (PNT), auxiliary verbs (AUX) and articles (ART), since we think these do not contribute much to the semantics of words.\n",
        "     2. replacing tokens with grammar types `ENT` (entities) and `NPH` (proper names) with generic variants `grammar#ENT` and `grammar#NPH` respectively. The rationale is that, depending on the input corpus, names of people or organizations may appear a few times, but may be filtered out if they do not appear enough times. This ensures such tokens are kept in the vocabulary and contribute to the embeddings of words nearby. The main disadvantage is that we will not have some proper names in our final vocabulary.\n",
        "\n",
        "```\n",
        "java $JAVA_OPTIONS net.expertsystem.word2vec.swivel.SubtokPrep \\\n",
        "  --input C:/hybridNLP2018/tutorial/datasamples/umbc_tlgs_wnscd_5K \\\n",
        "  --output_dir C:/corpora/umbc/coocs/tlgs_wnscd_5K_ls_f/  \\\n",
        "  --expected_seq_encoding TLGS_WN \\\n",
        "  --sub_tokens \\\n",
        "  --output_subtokens \"LEMMA,SYNSET\" \\\n",
        "  --remove_tokens_with_grammar_types \"PNT,AUX,ART\"  \\\n",
        "  --generalise_tokens_with_grammar_types \"ENT,NPH\" \\\n",
        "  --shard_size 512\n",
        "```\n",
        "\n",
        "The output log looked as follows:\n",
        "\n",
        "```\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - expected_seq_encoding set to 'TLGS_WN'\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - remove_tokens_with_grammar_types set to PNT,AUX,ART\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - generalise_tokens_with_grammar_types set to ENT,NPH\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - Creating vocab for C:\\hybridNLP2018\\tutorial\\datasamples\\umbc_tlgs_wnscd_5K\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - read 5000 lines from C:\\hybridNLP2018\\tutorial\\datasamples\\umbc_tlgs_wnscd_5K\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - filtered 166152 tokens from a total of 427796 (38,839%)\n",
        "generalised 1899 tokens from a total of 427796 (0,444%)\n",
        "full vocab size 21321\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - Vocabulary contains 5632 tokens (21321 full count, 5913 appear > 5 times)\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - Flushing 1279235 co-occ pairs\n",
        "INFO  net.expertsystem.word2vec.swivel.SubtokPrep - Wrote 121 tmpShards to disk\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "PcpcqXJ_OeBk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have included the output of this process as part of the GitHub repo for the tutorial. We will unzip this folder to inspect the results:"
      ]
    },
    {
      "metadata": {
        "id": "RRUEmfptOuIr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip /content/tutorial/datasamples/precomp-coocs-tlgs_wnscd_5K_ls_f.zip -d /content/umbc/coocs/\n",
        "precomp_coocs_path = '/content/umbc/coocs/tlgs_wnscd_5K_ls_f'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WwTFnSPTQBxl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The previous cell extracts the pre-computed co-occurrence shards and defines a variable `precomp_coocs_path` that points to the folder where these shards are stored.\n",
        "\n",
        "Next, we print the first 10 elements of the vocabulary to see the format that we are using to represent the lemmas and synsets:"
      ]
    },
    {
      "metadata": {
        "id": "uf6Vas-TP6h2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head -n10 {precomp_coocs_path}/row_vocab.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OvK8-CaBSOOO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As the output above shows, the vocabulary we get with `subtoken prep` is smaller (5.6K elements instead of over 8K) and it contains individual lemmas and synsets (it also contains *special* elements grammar#ENT and grammar#NPH, as described above).\n",
        "\n",
        "**More importantly**, the co-occurrence counts take into account the fact that certain lemmas co-occur more frequently with certain other lemmas and synsets, which should be taken into account when learning embedding representations."
      ]
    },
    {
      "metadata": {
        "id": "YE0VZd9lluKf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Learn embeddings from co-occurrence matrix\n",
        "\n",
        "With the sharded co-occurrence matrices created in the previous section it is now possible to learn embeddings by calling the `swivel.py` script. This launches a tensorflow application based on various parameters (most of which are self-explanatory) :\n",
        "\n",
        " - `input_base_path`: the folder with the co-occurrence matrix (protobuf files with the sparse matrix) generated above.\n",
        " - `submatrix_` rows and columns need to be the same size as the `shard_size` used in the `prep` step.\n",
        " - `num_epochs` the number of times to go through the input data (all the co-occurrences in the shards). We have found that for large corpora, the learning algorithm converges after a few epochs, while for smaller corpora you need a larger number of epochs. \n",
        " \n",
        " Execute the following cell to generate embeddings for the pre-computed co-occurrences."
      ]
    },
    {
      "metadata": {
        "id": "Adtnl3HWluKf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vec_path = '/content/umbc/vec/tlgs_wnscd_5k_ls_f'\n",
        "!python /content/tutorial/scripts/swivel/swivel.py --input_base_path={precomp_coocs_path} \\\n",
        "    --output_base_path={vec_path} \\\n",
        "    --num_epochs=40 --dim=150 \\\n",
        "    --submatrix_rows=512 --submatrix_cols=512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5JItR-RVluKl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This will take a few minutes, depending on your machine.\n",
        "The result is a list of files in the specified output folder, including:\n",
        " - the tensorflow graph, which defines the architecture of the model being trained\n",
        " - checkpoints of the model (intermediate snapshots of the weights)\n",
        " - `tsv` files for the final state of the column and row embeddings."
      ]
    },
    {
      "metadata": {
        "id": "FORz_vtGluKm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%ls {vec_path}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IrMmRcouluKq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert `tsv` files to `bin` file\n",
        "\n",
        "As we've seen in previous notebooks, the `tsv` files are easy to inspect, but they take too much space and they are slow to load since we need to convert the different values to floats and pack them as vectors. Swivel offers a utility to convert the `tsv` files into a `bin`ary format. At the same time it combines the column and row embeddings into a single space (it simply adds the two vectors for each word in the vocabulary)."
      ]
    },
    {
      "metadata": {
        "id": "IBtZG82kluKr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python /content/tutorial/scripts/swivel/text2bin.py --vocab={precomp_coocs_path}/row_vocab.txt --output={vec_path}/vecs.bin \\\n",
        "        {vec_path}/row_embedding.tsv \\\n",
        "        {vec_path}/col_embedding.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZXQOe0SWluKw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This adds the `vocab.txt` and `vecs.bin` to the folder with the vectors:"
      ]
    },
    {
      "metadata": {
        "id": "t1jV7uoGluKx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%ls {vec_path}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yL26TO9BluK4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inspect the embeddings\n",
        "\n",
        "As in previous notebooks, we can now use Swivel to inspect the vectors using the `Vecs` class. It accepts a `vocab_file` and a file for the binary serialization of the vectors (`vecs.bin`)."
      ]
    },
    {
      "metadata": {
        "id": "f3QA6U6nluK5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tutorial.scripts.swivel import vecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h3cXtjfYluK7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "...and we can load existing vectors. Here we load some pre-computed embeddings, but feel free to use the embeddings you computed by following the steps above (although, due to random initialization of weight during the training step, your results may be different)."
      ]
    },
    {
      "metadata": {
        "id": "e7VejLMMluK8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectors = vecs.Vecs(precomp_coocs_path + '/row_vocab.txt', \n",
        "            vec_path + '/vecs.bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LJCKmEpAluLB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, let's define a basic method for printing the `k` nearest neighbors for a given word:"
      ]
    },
    {
      "metadata": {
        "id": "V8dR-NxRluLH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And let's use the method on a few lemmas and synsets in the vocabulary:"
      ]
    },
    {
      "metadata": {
        "id": "AffhuN8SluLH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(vectors.k_neighbors('lem_California'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6IE6BC63j9KW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(vectors.k_neighbors('lem_semantic'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ffWNLUVHkJfr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(vectors.k_neighbors('lem_conference'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_K8HU-LDkjW-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(vectors.k_neighbors('wn31_conference.n.01'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_Q1p9DbluLM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that using the Vecsigrafo approach gets us very different results than when using standard swivel (notebook 01):\n",
        " * the results now include concepts (synsets), besides just words. Without further information, this makes interpreting the results harder since we now only have the concept id, but we can search for these concepts in the underlying KG (WordNet in this case) to explore the semantic network and get further information.\n",
        " \n",
        "Of course, results may not be very good, since these have been derived from a very small corpus (5K lines from UMBC). In the excercise below, we encourage you to download and inspect pre-computed embeddings based on the full UMBC corpus."
      ]
    },
    {
      "metadata": {
        "id": "PCG_FvU6luLM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(vectors.k_neighbors('lem_semantic web'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7rKAc6T_luLm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(vectors.k_neighbors('lem_ontology'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KAZGM9PcluLr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion and Exercises\n",
        "\n",
        "In this notebook we generated a vecsigrafo based on a disambiguated corpus. The resulting embedding space combines concept ids and lemmas.\n",
        "\n",
        "We have seen that the resulting space:\n",
        " 1. may be harder to inspect due to the potentially opaque concept ids\n",
        " 2. clearly different than standard swivel embeddings\n",
        " \n",
        "The question is: are the resulting embeddings *better*? \n",
        "\n",
        "To get an answer, in the next notebook, we will look at **evaluation methods for embeddings**."
      ]
    },
    {
      "metadata": {
        "id": "oOgdJ4BBgS-E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Explore full precomputed embeddings\n",
        "\n",
        "We have also pre-computed embeddings for the full UMBC corpus. The provided `tar.gz` file is about 1.1GB, hence downloading it may take several minutes."
      ]
    },
    {
      "metadata": {
        "id": "6bxeWC28gRzy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "full_precomp_url = 'https://zenodo.org/record/1446214/files/vecsigrafo_umbc_tlgs_ls_f_6e_160d_row_embedding.tar.gz'\n",
        "full_precomp_targz = '/content/umbc/vec/tlgs_wnscd_ls_f_6e_160d_row_embedding.tar.gz'\n",
        "!wget {full_precomp_url} -O {full_precomp_targz}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zr_9FdujqKMA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we have to unpack the vectors:"
      ]
    },
    {
      "metadata": {
        "id": "Xg1Og_PVnMZ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar -xzf {full_precomp_targz} -C /content/umbc/vec/\n",
        "full_precomp_vec_path = '/content/umbc/vec/vecsi_tlgs_wnscd_ls_f_6e_160d'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H8kHn673qBDW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%ls /content/umbc/vec/vecsi_tlgs_wnscd_ls_f_6e_160d/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xomWQ33ZugIr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data only includes the `tsv` version of the vectors, so we need to convert these to the binary format that Swivel uses. And for that, we aso need a `vocab.txt` file,  which we can derive from the tsv as follows:"
      ]
    },
    {
      "metadata": {
        "id": "TI08XC90tJW9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(full_precomp_vec_path + '/vocab.txt', 'w', encoding='utf_8') as f:\n",
        "  with open(full_precomp_vec_path + '/row_embedding.tsv', 'r', encoding='utf_8') as vec_lines:\n",
        "    vocab = [line.split('\\t')[0].strip() for line in vec_lines]\n",
        "    for word in vocab:\n",
        "      print(word, file=f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WvstUmD18uFh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's inspect the vocabulary:"
      ]
    },
    {
      "metadata": {
        "id": "lE0D6C5EuY5c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wc -l {full_precomp_vec_path}/vocab.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tfI5DsNZv3_6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!grep 'wn31_' {full_precomp_vec_path}/vocab.txt | wc -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hsaxAWIwv_AZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!grep 'lem_' {full_precomp_vec_path}/vocab.txt | wc -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n33cYIzvvy1P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we can see, the embeddings have a vocabulary of just under 1.5M entries, 56K of which are synsets and most of the rest are lemmas.\n",
        "\n",
        "Next, convert the `tsv` into the swivel's binary format. This can take a couple of minutes."
      ]
    },
    {
      "metadata": {
        "id": "RA_Nkam8wk4r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python /content/tutorial/scripts/swivel/text2bin.py --vocab={full_precomp_vec_path}/vocab.txt --output={full_precomp_vec_path}/vecs.bin \\\n",
        "        {full_precomp_vec_path}/row_embedding.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kqTmZaMOxp6g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we are ready to load the vectors."
      ]
    },
    {
      "metadata": {
        "id": "INgVB_9PwW7C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vecsi_wn_umbc = vecs.Vecs(full_precomp_vec_path + '/vocab.txt', \n",
        "            full_precomp_vec_path + '/vecs.bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ELtP4P4px_ao",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(vecsi_wn_umbc.k_neighbors('lem_California'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9KVBK500yL_q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(vecsi_wn_umbc.k_neighbors('lem_semantic'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DbgsE-ekyShq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(vecsi_wn_umbc.k_neighbors('lem_conference'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d3nPDt0TyeOZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(wn.synset('conference.n.01').definition())\n",
        "pd.DataFrame(vecsi_wn_umbc.k_neighbors('wn31_conference.n.01'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4p6Vubw5yg2K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(wn.synset('conference.n.03').definition())\n",
        "pd.DataFrame(vecsi_wn_umbc.k_neighbors('wn31_conference.n.03'))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}