{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_interlinking_and_curation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "p9jOxZnLRnfv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# KG curation, interlinking and multilinguality\n",
        "\n",
        "In this notebook we look at how embeddings can be used in curation of Knowledge Graphs, in particular in tasks such as graph completion and alignment."
      ]
    },
    {
      "metadata": {
        "id": "jSZqGQmCRnfw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## KG completion\n",
        "\n",
        "Knowledge Graph completion is the task of predicting whether an existing, incomplete, graph should add a vertix between two specific nodes. For example, in DBpedia, you may want to generate new links between pages and categories.\n",
        "\n",
        "Although embeddings for KGs are more suitable for this kind of task, word (and cross-modal) embeddings can also provide valuable input."
      ]
    },
    {
      "metadata": {
        "id": "lOg8pKEbRnfx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Multilingual KG alignment\n",
        "\n",
        "If you have multiple KGs that need to be aligned, you may be able to use *embedding alignment techniques*."
      ]
    },
    {
      "metadata": {
        "id": "BghVzkZBRnfy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Linear alignment\n",
        "The most straightforward alignment between two embedding spaces can be achieved by using a *translation matrix*.\n",
        "\n",
        "To calculate the translation matrix, you need a **dictionary** that provides mappings for a subset of your vocabularies.\n",
        "\n",
        "You can then use existing linear algorithms to calculate the pseudo inverse.\n",
        "\n",
        "For best results, it is recommended to use parallel corpora (so that the same words are encoded in similar ways) or very large corpora.\n",
        "\n",
        "In the following example, since we do not have a Spanish version of the kcap corpus, we use pre-generated embeddings for the most frequent 5K lemmas in the *United Nations parallel corpus*. We first load the vectors. "
      ]
    },
    {
      "metadata": {
        "id": "qkMJsaOeRnfz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/HybridNLP2018/tutorial.git\n",
        "from tutorial.scripts.swivel import vecs\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lYPurqdTRnf3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_path = '/content/tutorial/datasamples/UNv1.0/en_lemma_5k/'\n",
        "es_path = '/content/tutorial/datasamples/UNv1.0/es_lemma_5k/'\n",
        "en_vecs = vecs.Vecs(en_path + 'vocab.txt', \n",
        "            en_path + 'vecs.bin')\n",
        "es_vecs = vecs.Vecs(es_path + 'vocab.txt',\n",
        "            es_path + 'vecs.bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zLn1n1rgRnf9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's check a couple of words in each embedding space as we have done in previous notebooks:"
      ]
    },
    {
      "metadata": {
        "id": "e94qCmfMRngB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(en_vecs.k_neighbors('knowledge'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J-Qcdu4KRngF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(es_vecs.k_neighbors('conocimiento'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_tf7DdMjRngJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Besides the embeddings for English and Spanish, we also provide a **dictionary** that was generated automatically to map 1K English lemmas into Spanish."
      ]
    },
    {
      "metadata": {
        "id": "QARuMB5ZRngK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%ls /content/tutorial/datasamples/UNv1.0/\n",
        "en2es_dict_path = '/content/tutorial/datasamples/UNv1.0/en2es-lemma-dict-1k.txt'\n",
        "!head -n 5 {en2es_dict_path}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SY3EHhGyRngP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's load the dictionary into a python object."
      ]
    },
    {
      "metadata": {
        "id": "pp_O0DrtRngP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_dict(path, invert=False):\n",
        "    result = {}\n",
        "    with open(path, 'r') as lines:\n",
        "        for line in lines:\n",
        "            (key, val) = line.split(':')\n",
        "            if invert:\n",
        "                result[val.strip('\\n')] = key\n",
        "            else: \n",
        "                result[key] = val.strip('\\n')\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xFAxGEQIRngS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en2es = load_dict(en2es_dict_path)\n",
        "es2en = load_dict(en2es_dict_path, invert=True)\n",
        "len(en2es), len(es2en)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iTdEnUT2RngW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see from the reported numbers that some English lemmas were mapped to the same Spanish lemma."
      ]
    },
    {
      "metadata": {
        "id": "aInnQFzkRngX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's inspect some of the entries in the dictionary:"
      ]
    },
    {
      "metadata": {
        "id": "6QqB6oDbRngZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "min = 5\n",
        "max = min + 5\n",
        "for en in list(en2es)[min:max]:\n",
        "    print(en, '->', en2es[en])\n",
        "print('')\n",
        "for es in list(es2en)[min:max]:\n",
        "    print(es, '->', es2en[es])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5SFprFqeRngc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to create the translation matrix, we need to create two **aligned** matrices:\n",
        "  - $M_{en}$ will contain $n$ English embeddings from the dictionary\n",
        "  - $M_{es}$ will contain $n$ Spanish embeddings from the dictionary\n",
        "  \n",
        "However, since the dictionary was generated automatically, it may be the case that some of the entries in the dictionary are not in the English or Spanish vocabularies. We only need the `id`s in the respective `vecs`:"
      ]
    },
    {
      "metadata": {
        "id": "FuUGBGaSRngf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_dict_ids = []\n",
        "es_dict_ids = []\n",
        "es_dict_voc = []\n",
        "for es in es2en:\n",
        "    es_id = es_vecs.word_to_idx.get(es)\n",
        "    en_id = en_vecs.word_to_idx.get(es2en[es])\n",
        "    if en_id and es_id :\n",
        "        es_dict_voc.append(es)\n",
        "        en_dict_ids.append(en_id)\n",
        "        es_dict_ids.append(es_id)\n",
        "print(len(en_dict_ids), len(es_dict_ids))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JjhmKG2RRngk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From the 1K dictionary entries, only $477$ pairs were both in the English and the Spanish `vecs`. In order to verify that the translation works, we can split this into $450$ pairs that we will use to calculate the translation matrix and we keep the remaining $27$ for testing:"
      ]
    },
    {
      "metadata": {
        "id": "f2uSwzgcRngk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_en_dict_ids = en_dict_ids[:450]\n",
        "train_es_dict_ids = es_dict_ids[:450]\n",
        "test_en_ids = en_dict_ids[450:] \n",
        "test_es_ids = es_dict_ids[450:]\n",
        "print(len(train_en_dict_ids), len(test_en_ids))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bfXNUpslRngp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before calculating the translation matrix, let's verify that we need one. We chose 3 example words:\n",
        "  - *conocimiento*  and *proporcionar* are in the in the training set, \n",
        "  - *tema* is in the test set\n",
        "  \n",
        "For each word, we get:\n",
        " - the $5$ Spanish neighbors for the English vector\n",
        " - the $5$ Spanish neighbors for the Spanish translation according to the dictionary"
      ]
    },
    {
      "metadata": {
        "id": "xmVvj_UORngq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "es_examples = ['conocimientos', 'proporcionar', 'tema']\n",
        "from IPython.display import display\n",
        "for i, es in enumerate(es_examples):\n",
        "    print(es, '->', es2en[es])\n",
        "    print('top k for Spanish vector in English vector space:')\n",
        "    k = 5\n",
        "    df1 = pd.DataFrame(en_vecs.k_neighbors(es_vecs.lookup(es), k=k, result_key_suffix='_es_vec'))\n",
        "    print('top k for English translation in English vector space:')\n",
        "    df2 = pd.DataFrame(en_vecs.k_neighbors(es2en[es], k=k, result_key_suffix='_en'))\n",
        "    df3 = pd.concat([df1, df2], axis=1)\n",
        "    #print(df3)\n",
        "    display(df3)\n",
        "    print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q3B4UtsARngu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Clearly, simply using the Spanish vector in the English space does not work. Let's get the matrices:"
      ]
    },
    {
      "metadata": {
        "id": "_11t6J12Rngv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m_en = en_vecs.vecs[train_en_dict_ids]\n",
        "m_es = es_vecs.vecs[train_es_dict_ids]\n",
        "print(m_en.shape, m_es.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YrK4hSbTRngz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As expected, we get two matrices of $450 \\times 300$, since embeddings are of dimension $300$ and we have $450$ training examples. Now, we can calculate the translation matrix and define a method for linearly translating a point in the Spanish embedding space into a point in the English embedding space."
      ]
    },
    {
      "metadata": {
        "id": "R5N3n_GtRng0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tm_es2en = np.linalg.pinv(m_es).dot(m_en)\n",
        "def es_vec_to_en_vec(es_vec):\n",
        "    return np.dot(es_vec, tm_es2en)\n",
        "print(tm_es2en.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-V3UZ_mNRng4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we can see, the translation matrix is just a $300 \\times 300$ matrix.\n",
        "\n",
        "Now that we have the translation matrix, let's inspect the example words to see how it performs:"
      ]
    },
    {
      "metadata": {
        "id": "Ul7kLNcnRng5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i, es in enumerate(es_examples):\n",
        "    print(es, '->', es2en[es])\n",
        "    k = 5\n",
        "    print('\\t%s: Spanish vector for \"%s\" in English vector space' % ('es_vec', es))\n",
        "    df1 = pd.DataFrame(en_vecs.k_neighbors(es_vecs.lookup(es), k=k, result_key_suffix='_es_vec'))\n",
        "    print('\\t%s: English vector for \"%s\" in English vector space' % ('en', es2en[es]))\n",
        "    df2 = pd.DataFrame(en_vecs.k_neighbors(es2en[es], k=k, result_key_suffix='_en'))\n",
        "    print('\\t%s: Spanish vector for \"%s\" *mapped* to English vector space using tm_es2en' % ('tm_es_vec', es))\n",
        "    df3 = pd.DataFrame(en_vecs.k_neighbors(es_vec_to_en_vec(es_vecs.lookup(es)), k=k, result_key_suffix='_tm_es_vec'))\n",
        "    df4 = pd.concat([df1,df2,df3], axis=1)\n",
        "    display(df4)\n",
        "    print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-U1QXjdnRng9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Non-linear alignment\n",
        "\n",
        "The linear alignment seems to work OK for this set of embeddings. In our experience, when dealing with larger vocabularies (and vocabularies mixing lemmas and concepts), this approach does not scale, since the number of parameters is limited to the $d \\times d$ translation matrix.\n",
        "\n",
        "For such cases it is possible to follow the same approach, but instead of deriving a pseudo-inverse matrix, we train a neural network to learn a non-linear translation function. The non-linearities can be introduced by using activation functions such as ReLUs.\n",
        "\n",
        "See  [Towards a Vecsigrafo: Portable Semantics in Knowledge-based Text Analytics](https://pdfs.semanticscholar.org/b0d6/197940d8f1a5fa0d7474bd9a94bd9e44a0ee.pdf) for more details."
      ]
    },
    {
      "metadata": {
        "id": "Ia78X0o7Rng-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Example: Cross-modal embeddings\n",
        "\n",
        "In [Thoma, S., Rettinger, A., & Both, F. (2017). Towards Holistic Concept Representations: Embedding Relational Knowledge, Visual Attributes, and Distributional Word Semantics. In International Semantic Web Conference. Vienna, Austria.](https://pdfs.semanticscholar.org/413e/b0b519ac18ec86aaa290c86553291fae7ea2.pdf), a cross-modal embedding is generated for a 1538 concepts.  \n",
        "\n",
        "![Cross-modal embeddings](https://github.com/HybridNLP2018/tutorial/blob/master/images/cross-modal-embedding.PNG?raw=1)\n",
        "\n",
        "As part of their evaluations, the authors studied the problem of entity-type prediction (a subtask of KG completion), using a subgraph of DBpedia that provided coverage for the 1538 concepts. Their results were:\n",
        "\n",
        "![TriM1538 entity-type prediction results](https://github.com/HybridNLP2018/tutorial/blob/master/images/TriM1538-entity-type-pred-results.PNG?raw=1)\n",
        "\n",
        "The results show a clear improvement when using multi-modal embeddings, compared to just using the KG embeddings.\n",
        "\n",
        "In [notebook 08](https://colab.research.google.com/github/HybridNLP2018/tutorial/blob/master/08_scientific_information_management.ipynb) of this tutorial you will see another possible way of exploiting cross-modality."
      ]
    },
    {
      "metadata": {
        "id": "THw9FuX6YXC3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}