{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_fake_news.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "IbTcJaLbED_x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LvrdqB6oEImj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Application: fake news and deceptive language detection\n",
        "\n",
        "In this notebook, we will look at how we can use hybrid embeddings in the context of NLP tasks. In particular, we will see how to use and adapt deep learning architectures to take into account hybrid knowledge sources to classify documents. "
      ]
    },
    {
      "metadata": {
        "id": "Q8VbTo5YEu3R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Basic document classification using deep learning\n",
        "First, we will introduce a basic pipeline for training a deep learning model to perform text classification.\n",
        "\n",
        "### Dataset: deceptive language (fake hotel reviews)\n",
        "As a first dataset, we will use the [deceptive opnion spam](http://myleott.com/op-spam.html) dataset (See the exercises below for a couple of more challenging datasets on fake news detection).\n",
        "\n",
        "This corpus contains:\n",
        "  * 400 truthful positive reviews from TripAdvisor\n",
        "  * 400 deceptive positive reviews from Mechanical Turk\n",
        "  * 400 truthful negative reviews from Expedia, Hotels.com, Orbitz, Priceline, TripAdvisor and Yelp\n",
        "  * 400 deceptive negative reviews from Mechanical Turk\n",
        "  \n",
        "The dataset is described in more detail in the following papers:\n",
        "  \n",
        "  [M. Ott, Y. Choi, C. Cardie, and J.T. Hancock. 2011. Finding Deceptive Opinion Spam by Any Stretch of the Imagination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.](http://arxiv.org/abs/1107.4557)\n",
        "  \n",
        " [M. Ott, C. Cardie, and J.T. Hancock. 2013. Negative Deceptive Opinion Spam. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.](http://www.aclweb.org/anthology/N13-1053)\n",
        " \n",
        " For convenience, we have included the dataset as part of our GitHub tutorial repository."
      ]
    },
    {
      "metadata": {
        "id": "K0BdXx-kk8OX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hV-1NWZKEuLQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/HybridNLP2018/tutorial\n",
        "!head -n2 /content/tutorial/datasamples/deceptive-opinion.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7RN7FktWJf8O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The last two lines show that the dataset is distributed as a comma-separated-value file with various fields. For our purposes, we are only interested in fields:\n",
        " * `deceptive`: this can be either *truthful* or *deceptive*\n",
        " * `text`: the plain text of the review\n",
        " \n",
        "The other fields: `hotel` (name), `polarity` (positive or negative) and `source` (where the review comes from) are not relevant for us in this notebook.\n",
        "\n",
        "Let's first load the dataset in a format that is easier to feed into a text classification model. What we need is an object with fields:\n",
        "  * `texts`: an array of texts\n",
        "  * `categories`:  an array of textual tags (e.g. *truthful* or *deceptive*) \n",
        "  * `tags`: an array of integer tags (the categories)\n",
        "  * `id2tag`: a map from the integer identifier to the textual identifier for the tag\n",
        "  \n",
        "The following cell produces such an object:"
      ]
    },
    {
      "metadata": {
        "id": "lYpG3AxDlPku",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd # for handling tables a DataFrames\n",
        "import tutorial.scripts.classification as clsion # library for text classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ku3Q2fSYmA4N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hotel_df = pd.read_csv('/content/tutorial/datasamples/deceptive-opinion.csv',\n",
        "                   names=[\"deceptive\", \"hotel\", \"polarity\", \"source\", \"text\"])\n",
        "hotel_df = hotel_df[1:].reset_index() # first row is the header, so remove\n",
        "hotel_wnscd_df = pd.read_csv('/content/tutorial/datasamples/deceptive-opinion.tlgs_wnscd',\n",
        "                            names=['text_tlgs_wnscd'])\n",
        "hotel_df = pd.concat([hotel_df, hotel_wnscd_df], axis=1)\n",
        "raw_hotel_ds = clsion.read_classification_corpus(hotel_df, text_fields=['text'], tag_field='deceptive')\n",
        "raw_hotel_wnscd_ds = clsion.read_classification_corpus(hotel_df, text_fields=['text_tlgs_wnscd'], tag_field='deceptive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "urSPqyHsAwRk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The previous cell has actually loaded two versions of the dataset: \n",
        "  * `raw_hotel_ds` contains the actual texts as originally published\n",
        "  * `raw_hotel_wnscd_ds` provides the WordNet disambiguated `tlgs` tokenization (see notebooke 03 on Vecsigrafo for more details about this format). This is needed because we don't have a python method to automatically disambiguate a text using WordNet, so we provide this disambiguated version as part of the GitHub repo for this tutorial."
      ]
    },
    {
      "metadata": {
        "id": "NZXUzz379zMJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hotel_df[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "378QnYScBdlT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can print a couple of examples from both datasets. "
      ]
    },
    {
      "metadata": {
        "id": "Fl8fw58Qq5-l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clsion.sanity_check(raw_hotel_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ATDAu7x2CbLR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "clsion.sanity_check(raw_hotel_wnscd_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Esfd4PjoBnUs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Cleaning the raw text often produces better results; we can do this as follows:"
      ]
    },
    {
      "metadata": {
        "id": "VguaX9KcpF7b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cl_hotel_ds = clsion.clean_ds_texts(raw_hotel_ds)\n",
        "clsion.sanity_check(cl_hotel_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yTQz_UI-B2dh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenize and index the dataset\n",
        "\n",
        "As we said above, the raw datasets consist of `texts`, `categories` and `tags`. There are different ways to process the texts before passing it to a deep learning architecture, but typically they involve:\n",
        " * **tokenization**: how to split each document into basic forms which can be represented as vectors. In this notebook we will use tokenizations which result in words and synsets, but there are also architectures that accept character-level or n-grams of characters.\n",
        " * **indexing** of the text: in this step, the tokenized text is compared to a **vocabulary** (or, if no vocabulary is provided, it can be used to create a vocabulary),  a list of words, so that you can assign a unique integer identifier to each token. You need this so that tokens will then be represented as embedding or vectors in a matrix. So having an identifier will enable you to know which row in the matrix corresponds to which token in the vocabulary.\n",
        " \n",
        "The `clsion` library, included in the tutorial GitHub repo, already provides various indexing methods for text classification datasets. In the next cell we apply *simple indexing*, which uses white-space tokenization and creates a vocabulary based on the input dataset."
      ]
    },
    {
      "metadata": {
        "id": "XLoO9-y4sXmL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "csim_hotel_ds = clsion.simple_index_ds(cl_hotel_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DoOQHluDEfS_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since the vocabulary was created based on the dataset, all tokens in the dataset are also in the vocabulary. In the next sections, we will see examples where embeddings are provided during indexing. \n",
        "\n",
        "The following cell prints a couple of characteristics of the indexed dataset."
      ]
    },
    {
      "metadata": {
        "id": "LZTHq9exs5lu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\n",
        "    'vocab size:', len(csim_hotel_ds['vocab_embedding']['w2i']),\n",
        "    'dim:', csim_hotel_ds['vocab_embedding']['dim'],\n",
        "    'vectors:', csim_hotel_ds['vocab_embedding']['vecs'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I34TmO0KFRra",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we can see, the vocabulary is quite small (about 11K words). By default, it specifies that the vocabulary embeddings should be of dimention 150, but no vectors are specified. This means the model can assign random embeddings to the 11K words.\n",
        "\n",
        "### Define the experiment to run\n",
        "The `clsion` allows us to specify experiments to run: given an indexed dataset, we can execute a text classification experiment by specifying various hyper-parameters as follows:\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "O_kIzynFoWHQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "experiment1 = {\n",
        "    'hotel_csim': {\n",
        "        'indexed_dataset': csim_hotel_ds,\n",
        "        'executor': clsion.execute_experiment,\n",
        "        'hparams': clsion.merge_hparams([\n",
        "            clsion.common_hparams, clsion.biLSTM_hparams, \n",
        "            clsion.calc_hparams(csim_hotel_ds), \n",
        "            {    \n",
        "                'epochs': 20\n",
        "            }\n",
        "        ])\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uw7qMFdGGrAO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Under the hood, the library creates a Bidirectional LSTM model as requested (the library also can create other model architectures such as convolutional NNs).\n",
        "\n",
        "Since our dataset is fairly small, we don't need a very deep model. A fairly simple bidirectional LSTM should be sufficient. The generated model will consist of the following layers:\n",
        "  * The **input layer**: is a tensor of shape $(l, )$, where $l$ is the number of tokens for each document. The empty second parameter will let us pass the model different number of input documents, as long as they all have the same number of tokens.\n",
        "  * The **embedding layer** converts the each input document (a sequence of word ids) into a sequence of embeddings. Since we are not yet using pre-computed embeddings, these will be generated at random and trained with the rest of parameters in the model.\n",
        "  * The **lstm layer**s: one or more bidirectional LSTMs. Explaining these in detail is out of the scope of this tutorial. Suffice it to say, each layer goes through each embedding in the sequence and produces a new embedding taking into account previous and posterior embeddings. The final layer only produces a single embedding, which represents the full document.\n",
        "  * The **dense layer**: is a fully connected neural network that maps the output embedding of the final layer to a vector of 2 dimensions which can be compared to the manual labelled tag. \n",
        "  \n",
        "Finally, we can run our experiment using the `n_cross_val` method. Depending on whether you have an environment with a GPU this can be a bit slow, so we only train a model once. (In practice, model resuls may vary due to random initializations, so it's usually a good idea to run the same model several times to get an average evaluation  metric and an idea of how stable the model is.)"
      ]
    },
    {
      "metadata": {
        "id": "_8JNN9QJtqKc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ex1_df, ex1_best_run = clsion.n_cross_val(experiment1, n=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uiWDpfxuIChT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first element of the result is a DataFrame containing test results and a record of the used parameters."
      ]
    },
    {
      "metadata": {
        "id": "Y8BJQEtxybX9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ex1_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s2LJHg_wIWbR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Discussion\n",
        "\n",
        "Bidirectional LSTMs are really good at learning patters in text. However, this way of training a model will tend to overfit the training dataset. Since our dataset is fairly small and narrow: it only contains texts about hotel reviews, we should not expect this model to be able to detect fake reviews about other products or services. Similarly, we should not expect this model to be applicable to detecting other types of deceptive texts such as fake news.\n",
        "\n",
        "The reason why such a model is very tied to the training dataset is that even the vocabulary is derived from the dataset: it will be biased towards words (and senses of those words) related to hotel reviews. Vocabulary about other products, services and topics cannot be learned from the input dataset.\n",
        "\n",
        "Furthermore, since no pre-trained embeddings were used, the model had to learn the embedding weights from scratch based on the signal provided by the 'deceptive' tags. It did not have an opportunity to learn more generic relations between words from a wider corpus.\n",
        "\n",
        "For these reasons it is a good idea to use pre-trained embeddings as we show in the following sections."
      ]
    },
    {
      "metadata": {
        "id": "Cu1Hg4zy1bTj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using HolE embeddings\n",
        "\n",
        "In this section we use embeddings learned using `HolE` and trained on WordNet 3.0. As we have seen in previous notebooks, such embeddings capture the relations specified in the WordNet knowledge graph. As such, synset embeddings tend to encode useful knowledge. However, lemma embeddings tend to be of poorer quality."
      ]
    },
    {
      "metadata": {
        "id": "plkVeWPFKDNF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download the embeddings\n",
        "\n",
        "Execute the following cell to download and unpack the embeddings. If you recently executed previous notebooks as part of this tutorial, you may still have these in your environment."
      ]
    },
    {
      "metadata": {
        "id": "3bWsPSeEKf8a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir /content/vec/\n",
        "%cd /content/vec/\n",
        "!wget https://zenodo.org/record/1446214/files/wn-en-3.0-HolE-500e-150d.tar.gz\n",
        "!tar -xzf wn-en-3.0-HolE-500e-150d.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ULnwR7P-KwW2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%ls /content/vec/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tvbTuJHSKz6a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the embeddings and convert to the format expected by `clsion`\n",
        "\n",
        "The provided embeddings are in `swivel`'s binary + vocab format. However, the `clsion` library expects a different python datastructure. Furtheremore, it will be easier to match the lemmas in the dataset to plain text rather than the `lem_<lemma_word>` format used to encode the HolE vocabulary, hence we need to do some cleaning of the vocabulary. This occurs in the following cells:"
      ]
    },
    {
      "metadata": {
        "id": "GRoCV1uiMRok",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tutorial.scripts.swivel.vecs as vecs\n",
        "vocab_file = '/content/vec/wn-en-3.1-HolE-500e.vocab.txt'\n",
        "holE_voc_file = '/content/vec/wn-en-3.1-HolE-500e.clean.vocab.txt'\n",
        "with open(holE_voc_file, 'w', encoding='utf_8') as wf:\n",
        "  with open(vocab_file, 'r', encoding='utf_8') as f:\n",
        "    for word in f.readlines():\n",
        "      word = word.strip()\n",
        "      if not word:\n",
        "        continue\n",
        "      if word.startswith('lem_'):\n",
        "        word = word.replace('lem_', '').replace('_', ' ')\n",
        "      print(word, file=wf)\n",
        "vecbin = '/content/vec/wn-en-3.1-HolE-500e.tsv.bin'\n",
        "wnHolE = vecs.Vecs(holE_voc_file, vecbin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qD-dOeC0P2sO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import array\n",
        "import tutorial.scripts.swivel.vecs as vecs\n",
        "\n",
        "def load_swivel_bin_vocab_embeddings(bin_file, vocab_file):\n",
        "    vectors = vecs.Vecs(vocab_file, bin_file)\n",
        "    vecarr = array.array(str('d'))\n",
        "    for idx in range(len(vectors.vocab)):\n",
        "        vec = vectors.vecs[idx].tolist()[0]\n",
        "        vecarr.extend(float(x) for x in vec)\n",
        "    return {'itos': vectors.vocab,\n",
        "            'stoi': vectors.word_to_idx,\n",
        "            'vecs': vecarr,\n",
        "            'source': 'swivel' + bin_file,\n",
        "            'dim': vectors.vecs.shape[1]}\n",
        "wnHolE_emb=load_swivel_bin_vocab_embeddings(vecbin, holE_voc_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EpVaxoPLLyOS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we have the WordNet HolE embedding in the right format, we can explore some of the 'words' in the vocabulary:"
      ]
    },
    {
      "metadata": {
        "id": "Tq8TJ1efSydt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wnHolE_emb['itos'][150000] # integer to string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "70yxIgqNL8Xi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenize and index the dataset\n",
        "As in the previous case, we need to tokenize the raw dataset. However, since we now have access to the WordNet HolE embeddings, it make sense to use the WordNet disambiguated version of the text (i.e. `raw_hotel_wnscd_ds`).  The `clsion` library already provides a method `index_ds_wnet` to perform tokenization and indexing using the expected WordNet encoding for synsets. "
      ]
    },
    {
      "metadata": {
        "id": "D7kK2ZbAVfRM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wn_hotel_ds = clsion.index_ds_wnet(raw_hotel_wnscd_ds, wnHolE_emb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uwzq_ktfOhEv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\n",
        "    'vocab size:', len(wn_hotel_ds['vocab_embedding']['w2i']),\n",
        "    'dim:', wn_hotel_ds['vocab_embedding']['dim'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LueZDqMTMsCb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The above produces an `ls` tokenization of the input text, which means that each original token is mapped to both a lemma and a synset. The model will then use both of these to map each token to the concatenation of the lemma and synset embedding. Since the WordNet HolE has 150 dimensions, each token will be represented by a 300 dimensional embedding (the concatenation of the lemma and synset embedding).\n",
        "\n",
        "### Define the experiment and run\n",
        "We define the experiment using this new dataset as follows, the main change is that we do not want the embedding layer to be trainable, since we want to maintain the knowledge learned via HolE from WordNet. The model should only train the LSTM and dense layers to predict whether the input text is deceptive or not."
      ]
    },
    {
      "metadata": {
        "id": "mdi3QHAmWODm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "experiment2 = {\n",
        "    'hotel_wn_holE': {\n",
        "        'indexed_dataset': wn_hotel_ds,\n",
        "        'executor': clsion.execute_experiment,\n",
        "        'hparams': clsion.merge_hparams([\n",
        "            clsion.common_hparams, clsion.biLSTM_hparams, \n",
        "            clsion.calc_hparams(wn_hotel_ds), \n",
        "            {    \n",
        "                'epochs': 20,\n",
        "                'emb_trainable': False\n",
        "            }\n",
        "        ])\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y8ek9NEBWchy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ex2_df, ex2_best_run = clsion.n_cross_val(experiment2, n=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q06jci6yODpj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ex2_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZTr3SfqcN_8Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Discussion\n",
        "Although the model performs worse than the `csim` version, we can expect the model to be applicable to closely related domains. The hope is that, even if words did not appear in the training dataset, the model will be able to exploit embedding similarities learned from WordNet to generalise the 'deceptive' classification."
      ]
    },
    {
      "metadata": {
        "id": "oKjrRR9CYMVC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using Vecsigrafo UMBC WNet embeddings\n"
      ]
    },
    {
      "metadata": {
        "id": "UrvfGKH8Paac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download the embeddings\n",
        "\n",
        "If you executed previous notebooks, you may already have the embedding in your environment."
      ]
    },
    {
      "metadata": {
        "id": "ijEmWlb9YYzq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%mkdir /content/umbc\n",
        "%mkdir /content/umbc/vec\n",
        "full_precomp_url = 'https://zenodo.org/record/1446214/files/vecsigrafo_umbc_tlgs_ls_f_6e_160d_row_embedding.tar.gz'\n",
        "full_precomp_targz = '/content/umbc/vec/tlgs_wnscd_ls_f_6e_160d_row_embedding.tar.gz'\n",
        "!wget {full_precomp_url} -O {full_precomp_targz}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jeSSepo6ZiaT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar -xzf {full_precomp_targz} -C /content/umbc/vec/\n",
        "full_precomp_vec_path = '/content/umbc/vec/vecsi_tlgs_wnscd_ls_f_6e_160d'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xe2zxC0DZqB0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%ls /content/umbc/vec/vecsi_tlgs_wnscd_ls_f_6e_160d/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TIppmSLhPw3I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since the embeddings were distributed as `tsv` files, we can use the `load_tsv_embeddings` method. Training models with all 1.4M vocab elements requires a lot of RAM, so we limit ourselves to only the first 250K vocab elements (these are the most frequent lemmas and synsets in UMBC)."
      ]
    },
    {
      "metadata": {
        "id": "iAGZzstpZNNq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def simple_lemmas(word):\n",
        "  if word.startswith('lem_'):\n",
        "    return word.replace('lem_', '').replace('_', ' ')\n",
        "  else:\n",
        "    return word\n",
        "    \n",
        "wn_vecsi_umbc_emb = clsion.load_tsv_embeddings(full_precomp_vec_path + '/row_embedding.tsv', \n",
        "                                               max_words=250000,\n",
        "                                               word_map_fn=simple_lemmas\n",
        "                                              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UZjpxS_jQZSf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenize and index dataset"
      ]
    },
    {
      "metadata": {
        "id": "lSsMW1q8a9k3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wn_v_umbc_hotel_ds = clsion.index_ds_wnet(raw_hotel_wnscd_ds, wn_vecsi_umbc_emb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m6ZwpItgQfiW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\n",
        "    'vocab size:', len(wn_v_umbc_hotel_ds['vocab_embedding']['w2i']),\n",
        "    'dim:', wn_v_umbc_hotel_ds['vocab_embedding']['dim'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W6YX784-QkvS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the experiment and run"
      ]
    },
    {
      "metadata": {
        "id": "xcMsuJPDbL-N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "experiment3 = {\n",
        "    'hotel_wn_vecsi_umbc': {\n",
        "        'indexed_dataset': wn_v_umbc_hotel_ds,\n",
        "        'executor': clsion.execute_experiment,\n",
        "        'hparams': clsion.merge_hparams([\n",
        "            clsion.common_hparams, clsion.biLSTM_hparams, \n",
        "            clsion.calc_hparams(wn_v_umbc_hotel_ds), \n",
        "            {    \n",
        "                'epochs': 20,\n",
        "                'emb_trainable': False\n",
        "            }\n",
        "        ])\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oKi3m1Mdbbti",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ex3_df, ex3_best_run = clsion.n_cross_val(experiment3, n=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qh0ltLkZQn2l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ex3_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zzba6KGtc4cZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Combine HolE and UMBC embeddings\n",
        "One of the advantages of embeddings as a knowledge representation device is that they are trivial to combine. In the previous experiments we have tried to use lemma and synset embeddings derived from:\n",
        "  * WordNet via HolE: these embeddings *encode* the knowledge derived from the structure of the WordNet Knowledge Graph\n",
        "  * the Shallow Connectivity disambiguation of the UMBC corpus: these embeddings *encode* the knowledge derived from trying to predict the lemmas and synsets from their contexts.\n",
        "  \n",
        "Since the embeddings encode different types of knowledge, it can be useful to use both embeddings at the same time when passing them to the deep learning model, as shown in this section."
      ]
    },
    {
      "metadata": {
        "id": "YOz3GjVsHHdb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Combine the embeddings\n",
        "We use the `concat_embs` method, which will go through the vocabularies of both input embeddings and concatenate them. Missing embeddings from one vocabulary will be mapped to the zero vector. Note that since `wnHolE_emb` has dimension 150 and `wn_vecsi_umbc_emb` has dimension 160, the resulting embedding will have dimension 310. (Besides concatenation, you could also experiment with other merging operations such as summation, substraction or averaging of the embeddings)."
      ]
    },
    {
      "metadata": {
        "id": "JJAl__5Hc98f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wn_vh_emb = clsion.concat_embs(wn_vecsi_umbc_emb, wnHolE_emb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z41Lg-2LenEE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "synsets =  [w for w in wn_vh_emb['itos'] if w.startswith('wn31_')]\n",
        "print('vocab has ', len(wn_vh_emb['itos']), '\"words\"', len(synsets), 'of which are synsets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d3LuR2lafJ0A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wn_vh_hotel_ds = clsion.index_ds_wnet(raw_hotel_wnscd_ds, wn_vh_emb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sCN5vmw5flGc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "experiment4 = {\n",
        "    'hotel_wn_vecsi_umbc': {\n",
        "        'indexed_dataset': wn_vh_hotel_ds,\n",
        "        'executor': clsion.execute_experiment,\n",
        "        'hparams': clsion.merge_hparams([\n",
        "            clsion.common_hparams, clsion.biLSTM_hparams, \n",
        "            clsion.calc_hparams(wn_vh_hotel_ds), \n",
        "            {    \n",
        "                'epochs': 20,\n",
        "                'emb_trainable': False\n",
        "            }\n",
        "        ])\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tZALCHc7fvPv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ex4_df, _ = clsion.n_cross_val(experiment4, n=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YTf13h_1jKV2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Discussion and results\n",
        "In this notebook we have shown how to use use different types of embeddings as part of a deep learning text classification pipeline. We have not performed detailed experiments on the WordNet-based embeddings used in this notebook and, because the dataset is fairly small, the results can have quite a bit of variance depending on the initialization parameters. However, we have performed studies based on Cogito-based embeddings. The tables below shows some of our results:\n",
        "\n",
        "The first set of results correspond to experiment 1 above. We trained the embeddings but explored various tokenizations strategies. \n",
        "\n",
        "\n",
        " | code      | $\\mu$ acc | $\\sigma$ acc | tok         | vocab | emb                 | trainable             |\n",
        " | -------   | --------- | ------------ | ----------- | ----- | ------------------- | --------------------- |\n",
        " | sim       |  0.8200   | 0.023        | ws          | ds    | random              | y                     | \n",
        " | tok       |  0.8325   | 0.029        | keras       | ds    | random              | y                     | \n",
        " | csim      |  0.8513   | 0.014        | clean ws    | ds    | random              | y                     | \n",
        " | ctok      |  0.8475   | 0.026        | clean keras | ds    | random              | y                     | \n",
        "\n",
        "As discussed above, this approach produces the best test results, but the trained models are very specific to the training dataset. The current practice is therefore to use pre-trained word-embeddings. FastText embeddings tend to yield the best performance. We got the following results.\n",
        "\n",
        " | code      | $\\mu$ acc | $\\sigma$ acc | tok         | vocab | emb                 | trainable             |\n",
        " | -------   | --------- | ------------ | ----------- | ----- | ------------------- | --------------------- |\n",
        "| ft-wiki   |  0.7356   | 0.042        | ws          | 250K  | `wiki-en.vec`       | n                     |\n",
        " | ft-wiki   |  0.7775   | 0.044        | clean ws    | 250K  | `wiki-en.vec`       | n                     |\n",
        " \n",
        " Next, we tried using HolE embedding trained on sensigrafo 14.2, which had very poor results:\n",
        " \n",
        " | code      | $\\mu$ acc | $\\sigma$ acc | tok         | vocab | emb                 | trainable             |\n",
        " | -------   | --------- | ------------ | ----------- | ----- | ------------------- | --------------------- | \n",
        " | HolE_sensi   |  0.6512   | 0.044        | cogito `s`  | 250K  | `HolE-en.14.2_500e` | n                    |\n",
        "\n",
        "Next we tried vecsigrafo trained on both wikipedia and umbc, either using only lemmas, only syncons or both lemmas and syncons. Using both lemmas and syncons always is better.\n",
        "\n",
        " | code      | $\\mu$ acc | $\\sigma$ acc | tok         | vocab | emb                 | trainable             |\n",
        " | -------   | --------- | ------------ | ----------- | ----- | ------------------- | --------------------- | \n",
        "| v_wiki_l  |  0.7450   | 0.050        | cogito `l`  | 250K  | `tlgs_ls_f_6e_160d` | n                     |\n",
        " | v_wiki_s  |  0.7363   | 0.039        | cogito `s`  | 250K  | `tlgs_ls_f_6e_160d` | n                     |\n",
        " | v_wiki_ls |  0.7450   | 0.032        | cogito `ls` | 250K  | `tlgs_ls_f_6e_160d` | n                     |\n",
        " | v_umbc_ls |  0.7413   | 0.038        | cogito `ls` | 250K  | `tlgs_ls_6e_160d`   | n                     |\n",
        " | v_umbc_l  |  0.7350   | 0.041        | cogito `l`  | 250K  | `tlgs_ls_6e_160d`   | n                     |\n",
        " | v_umbc_s  |  0.7606   | 0.032        | cogito `s`  | 250K  | `tlgs_ls_6e_160d`   | n                     |\n",
        "\n",
        "\n",
        "Finally, like in the experiment 4 above, we concatenated vecsigrafos (both lemmas and syncons) with HolE embeddings (only syncons, since lemmas tend to be poor quality). This produced the best results with a mean test accuracy of 79.31%. This is still lower than `csim`, but we expect this model to be more generic and applicable to other domains besides hotel reviews.\n",
        "\n",
        " | code      | $\\mu$ acc | $\\sigma$ acc | tok         | vocab | emb                 | trainable             |\n",
        " | -------   | --------- | ------------ | ----------- | ----- | ------------------- | --------------------- | \n",
        "| vw_H_s    |  0.7413   | 0.033        | cogito `s`  | 304K  | `tlgs_lsf`, `HolE`  | n                     |\n",
        " | vw_H_ls   |  0.7213   | 0.067        | cogito `ls` | 250K  | `tlgs_lsf`, `HolE`  | n                     |\n",
        " | vw_ls_H_s |  0.7275   | 0.041        | cogito `ls` | 250K  | `tlgs_lsf`, `HolE`  | n                     |\n",
        " | vu_H_s    |  0.7669   | 0.043        | cogito `s`  | 309K  | `tlgs_ls`, `HolE`   | n                     |\n",
        " | vu_ls_H_s |  0.7188   | 0.043        | cogito `ls` | 250K  | `tlgs_ls`, `HolE`   | n                     |\n",
        " | vu_ls_H_s |  0.7225   | 0.033        | cogito `l`  | 250K  | `tlgs_ls`, `HolE`   | n                     |\n",
        " | vu_ls_H_s |  0.7788   | 0.033        | cogito `s`  | 250K  | `tlgs_ls`, `HolE`   | n                     |\n",
        " | vu_ls_H_s |  0.7800   | 0.035        | cl cog `s`  | 250K  | `tlgs_ls`, `HolE`   | n                     |\n",
        " | vu_ls_H_s |  0.7644   | 0.044        | cl cog `l`  | 250K  | `tlgs_ls`, `HolE`   | n                     |\n",
        " | vu_ls_H_s |**0.7931** | 0.045        | cl cog `ls` | 250K  | `tlgs_ls`, `HolE`   | n                     |\n",
        " | vu_ls_H_s |  0.7838   | 0.028        | cl cog `s`  | 500K  | `tlgs_ls`, `HolE`   | n                     |\n",
        " | vu_ls_H_s |  ?        |  ?           | cl cog `l`  | 500K  | `tlgs_ls`, `HolE`   | n                     |\n",
        " | vu_ls_H_s |  0.7819   | 0.035        | cl cog `ls` | 500K  | `tlgs_ls`, `HolE`   | n                     |\n",
        " \n",
        " Finally, we have also experimented with a new type of embeddings, called contextual embeddings. Described in [Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep contextualized word representations. ](http://arxiv.org/abs/1802.05365). However, we did not manage to reproduce good results with this approach. \n",
        " \n",
        "| code      | $\\mu$ acc | $\\sigma$ acc | tok         | vocab | emb                 | trainable             |\n",
        " | -------   | --------- | ------------ | ----------- | ----- | ------------------- | --------------------- |  \n",
        " | elmo      |  0.7250   | 0.039        | nltk sent   | $\\infty$ | `elmo-5.5B`      | n (0.1 dropout)       |\n",
        " | elmo      |  0.7269   | 0.038        | nltk sent   | $\\infty$ | `elmo-5.5B`      | n (0.5 dropout, 20ep) |\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "izODAcA2Iciv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Further Exercises\n",
        "\n",
        "## Use the `fake_news` dataset from UMichigan\n"
      ]
    },
    {
      "metadata": {
        "id": "SNvd7iLSW3_v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}